\section{Linear Regression}\label{sec:linreg}
Suppose you have a data set $\mathcal{L}$ consisting of the data
$\mathbf{X}_\mathcal{L}=\{(y_i, x_i), i=0\ldots n-1\}$. Each point is associated
with a scalar target $y_i$, and a vector $\bs{x}$ containing values for $p$ input
features. Assuming the target variable $y_i$ is linear in the inputs, it can be
written as a linear function of the features, given by
\begin{equation}\label{eq:ols-target}
y_i = w_0 x_{i,0} + w_1 x_{i,1} + ... + w_{p-1} x_{i, p-1} + \epsilon_i,
\end{equation}
where $\bs{w} = (w_0, w_1, \dots, w_{p-1})^T$ is a vector of length
$p$ containing unknown values, and $\epsilon$ are the errors in our estimate.
This gives us a system of linear equations, which can be written in matrix form as
\begin{equation}\label{eq:ols-target-mat}
  \bs{y} = \bs{Xw} + \bs{}{\epsilon},
\end{equation}
where
\begin{equation}
\vec{X} = \left[
\begin{matrix}
x_{0,0} & x_{0,1} & x_{0,2} & ... & x_{0,p-1}\\
x_{1,0} & x_{1,1} & x_{1,2} & ... & x_{1,p-1}\\
x_{2,0} & x_{2,1} & x_{2,2} & ... & x_{2,p-1}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n-1,0} & x_{n-1,1} & x_{n-1,2} & ... & x_{n-1,p-1}
\end{matrix}
\right]
\end{equation}
The unknown values $\bs{w}$ are commonly referred to as \textit{weights} in machine 
learning literature. To find the best possible weights $\bs{w}$ we want a suitable
quantity to optimize - a \textbf{cost function}, $\mathcal{C}$ (also referred to as an
\textbf{objective function}). An example of such a function is the squared error - or the Euclidian vector norm, defined as
\begin{equation}
	L_2(\bs{x}) =||\bs{x}||_2 = \left(\sum x_i^2\right)^{\frac{1}{2}}.
\end{equation}
From this we define the cost function
\begin{equation}
	\mathcal{C} = || \bs{\hat{y}} - \bs{y} ||_2 ^2.
\end{equation}
In machine learning, it is most common to cast the optimization as a minimization problem
("minimize the cost"). Our task is then to find an approximation
\begin{equation}\label{eq:ols-approximation}
	\bs{\hat{y}} = \bs{Xw}
\end{equation}
which minimizes this cost function.
To find the minimum we need a differentiation. To simplify that process, we rewrite the cost function on matrix form
\begin{align*}
\mathcal{C} &= || \bs{\hat{y}} - \bs{y} ||_2 ^2, \\
\mathcal{C} &= ( \bs{\hat{y}} - \bs{Xw})^T( \bs{\hat{y}} - \bs{Xw}).
\end{align*}
To minimize we take the derivative with respect to the weights $\bs{w}$,
and find the minima by setting the derivative equal to zero
\begin{align}
\nabla _{\bs{w}}\mathcal{C} &= \nabla _{\bs{w}} ( \bs{\hat{y}} - \bs{Xw})^T( \bs{\hat{y}} - \bs{Xw}), \\
&= -2\bs{X}^T\bs{\hat{y}} + 2\bs{X}^T\bs{X}\bs{w}, \\
\bs{0} &= -2\bs{X}^T\bs{\hat{y}} + 2\bs{X}^T\bs{X}\bs{w}, \\
\bs{X}^T\bs{\hat{y}} &= \bs{X}^T\bs{X}\bs{w}, \\
\bs{w} &=(\bs{X}^T\bs{X})^{-1} \bs{X}^T\bs{\hat{y}}. \label{eq:ols}
\end{align}
This requires matrix \(\vec{X}^T\vec{X}\) to be invertible to get the solution 
\cite{James2000}.

The residuals $\bs{\epsilon}$ are given by
$$\bs{\epsilon} = \bs{y}-\bs{\hat{y}} = \bs{y}-\bs{X}\bs{w},$$
and with 
$$\bs{X}^T\left( \bs{y}-\bs{X}\bs{w}\right)= 0,$$
we have
$$\bs{X}^T\bs{\epsilon}=\bs{X}^T\left( \bs{y}-\bs{X}\bs{w}\right)= 0,$$
meaning that the solution for $\bs{w}$ is the one which minimizes the residuals.
This method of regression is known as Ordinary Least Squares.

\section{Over- and underfitting}\label{sec:overfitting}
In machine learning, when fitting a model to a data set the goal is nearly always to predict 
values or classify samples from regions of data the model has not seen. This is not a simple task,
especially taking into consideration that data is rarely, if ever, noiseless. When extrapolating to 
unseen regions we must take steps to ensure the model complexity is appropriate - we want it to fit 
the signal, not the noise. First off - what do the terms "overfit" and "underfit" mean?
An overfit model will typically perform well during the fitting procedure, but when presented with
data outside the fitted region its performance decreases considerably.
An underfit model lacks the expressive power to capture core signal variations in the data.
Mehta et. al \cite{Mehta2019} demonstrates this concept through polynomial regression.

In machine learning literature and practice, you will encounter the concept of
splitting the available data into two - training data and test data. We fit, or 'train'
the model on the training data, and then assess the performance of the model on the 
test data. This practice lets us evaluate whether the model is overfitting to unseen
data by comparing performance on the training data and test data.

\section{The Bias-Variance Tradeoff}\label{sec:bias-variance}
Considering the same dataset $\mathcal{L}$ consisting of the data,
let us assume that the true data is generated from a noisy model

$$\mathbf{y}=f(\bs{x}) + \bs{\epsilon},$$
where $\epsilon$ is normally distributed with mean zero and standard deviation $\sigma^2$.

In our derivation of the ordinary least squares method we defined
an approximation (\ref{eq:ols-approximation}) to the function $f$ in terms of the 
weights $\bs{w}$ and the input matrix $\bs{X}$ which together define our model,
that is $\bs{\hat{y}}=\bs{X}\bs{w}$. 

Thereafter we found the parameters $\bs{w}$ by optimizing the means squared error via the so-called cost function

$$\mathcal{C}(\bs{X},\bs{w}) =\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\hat{y}_i)^2=\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right].$$

We can rewrite this as 
$$\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right]=\frac{1}{n}\sum_i(f_i-\mathbb{E}\left[\bs{\hat{y}}\right])^2+\frac{1}{n}\sum_i(\hat{y}_i-\mathbb{E}\left[\bs{\hat{y}}\right])^2+\sigma^2.$$

The three terms represent the square of the bias of the learning
method, which can be thought of as the error caused by the simplifying
assumptions built into the method. The second term represents the
variance of the chosen model and finally the last terms is variance of
the error $\bs{\epsilon}$.

For a derivation of this equation, we refer to the work of Mehta et. al \cite{Mehta2019}.

% Derivation from lecture notes here.
%to recall that the variance of $\bs{y}$ and $\bs{\epsilon}$ are both equal to $\sigma^2$. 
%The mean value of $\bs{\epsilon}$ is by definition equal to zero. Furthermore, the function $f$ is not a stochastics variable, idem for $\bs{\hat{y}}$.
%We use a more compact notation in terms of the expectation value
%$$\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right]=\mathbb{E}\left[(\bs{f}+\bs{\epsilon}-\bs{\hat{y}})^2\right],$$
%and adding and subtracting $\mathbb{E}\left[\bs{\hat{y}}\right]$ we get
%$$\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right]=\mathbb{E}\left[(\bs{f}+\bs{\epsilon}-\bs{\hat{y}}+\mathbb{E}\left[\bs{\hat{y}}\right]-\mathbb{E}\left[\bs{\hat{y}}\right])^2\right],$$
%which, using the abovementioned expectation values can be rewritten as
%$$\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right]=\mathbb{E}\left[(\bs{y}-\mathbb{E}\left[\bs{\hat{y}}\right])^2\right]+\mathrm{Var}\left[\bs{\hat{y}}\right]+\sigma^2,$$
%that is the rewriting in terms of the so-called bias, 
%the variance of the model $\bs{\hat{y}}$ and the variance of $\bs{\epsilon}$.

\section{Regularization}\label{section:regularization}
With the computing resources available today, increasing model complexity to deal 
with underfitting is usually a simple task. However, this computational freedom 
has led to overfitting being the common challenge to overcome.

\section{Logistic Regression}\label{seq:logistic}
Differently to linear regression, classification problems
are concerned with outcomes taking the form of discrete variables.
For a specific physical problem, we'd like to identify its state, say whether
it is an ordered of disordered system. (cite?) One of the most basic examples
of a classifier algorithm is logistic regression.

For a logistic regressor to improve it needs a way to
track its performance. This is the purpose of a cost function. Essentially,
the cost function says something about how wrong the model is in classifying the
input. The objective in machine learning, and logistic regression, is then to minimize
this error.

The cost function used in this project is called the \textbf{cross-entropy}, or the
'negative log likelihood', and takes the form
\begin{equation}\label{eq:cross-entropy}
	\mathcal{C}(\bs{w})=-\sum_{i=1}^n  \left(y_i(w_0+w_1x_i) -\log{(1+\exp{(w_0+w_1x_i)})}\right)
\end{equation}

\section{Gradient Descent}\label{sec:gradient-descent}
Finding the minima or maxima of a functions is a well-known process, perhaps especially
so in physics. In machine learning most, if not all, cost optimization problems are
cast as minimization problems, and we will do the same here.

The basic idea of gradient descent is that a function $F(x)$, $x\equiv(x_1,\dots,x_n)$,
decreases fastest if one goes from x in the direction of the negative gradient $-\nabla F(x)$.
It can be shown that if
$$x_{k+1} = x_k - \gamma_k \nabla F(x_k),$$
with $\gamma_k > 0$, then for $\gamma_k$ small enough, $F(x_{k+1}) \leq F(x_k)$.
This means that for a sufficiently small $\gamma_k$ we are always moving 
towards smaller function values, i.e a minimum. The first point, $x_0$, is an
initial guess for the minimum. It could be chosen at random, or you could exploit some
prior knowledge if available. The parameter $\gamma_k$ is often
referred to as step length or \textit{learning rate}. We will be using the latter 
term in this thesis.

Ideally the sequence ${\bs{x}_k}_{k=0}$ converges to a global minimum of the function 
$F$. We do not generally know if the minimum we find is local or global,
unless we have the special case where $F$ is a convex function. In this case all
local minima are global minima, and gradient descent can converge to the global
solution. However, gradient descent is sensitive to the choice of learning rate $\gamma_k$.
As mentioned above $F(x_{k+1}) \leq F(x_k)$ is only guaranteed for sufficiently small
$\gamma_k$. If the learning rate is too small the method will converge slowly. If it
is too large we can experience erratic behaviour.

\subsection{Stochastic Gradient Descent}\label{sec:SGD}
The stochastic gradient descent (SGD) method address some of the shortcomings
of the normal gradient descent method by introducing randomness.
The cost function we wish to optimize can almost always be expressed as a sum
over $n$ data points $\{x_i\}_{i=1}^n$.
\begin{equation}
	\mathcal{C}(\bs{w}) = \sum\limits_{i=1}^n c_i (\bs{x}_i\bs{w})
\end{equation}
Which gives us the ability to find the gradient as a sum over $i$ gradients
\begin{equation}
	\nabla_{\bs{w}} \mathcal{C}(\bs{w}) = \sum\limits_{i}^n \nabla_{\bs{w}}c_i (\bs{x}_i\bs{w})
\end{equation}
Stochasticity/randomness is included by only taking the gradient on a subset of data
called \textit{minibatches}. Let the size of each minibatch be denoted $M$. Given a
set of n datapoints, this gives us $n/M$ minibatches, which we will denote $B_k$,
with $k = 1, \dots, n/M$.

Now, the procedure for calculating the gradient is now an approximation. Instead of
summing over all data points, we sum over the data points in one minibatch, chosen
at random each gradient step. This means that one gradient step is now given by
\begin{equation}\label{eq:sgd}
	w_{j+1} = w_j - \gamma_j \sum_{i\in B_k}^{n}\nabla_{\bs{w}}c_i(\bs{x}_i, w)
\end{equation}
where $k$ is chosen at random with equal probability from $[1, n/M]$.
Iterating over the number of minibatches is commonly referred to as an \textit{epoch}.
When training a model it is typical to choose the number of epochs and then iterate
over the number of minibatches each epoch. There are two important gains from this 
introduced stochasticity.
\begin{itemize}
	\item Decreased chance that our optimization scheme gets stuck in a local minima.
	\item If the size of each minibatch is small relative to the number of datapoints,
	the computation of the gradient is much cheaper.
\end{itemize}

\subsection{adam}
Optimization of the training process has been a focus in the machine learning
community. Introduced by Kingma and Lei Ba \cite{Kingma2015}, \textit{adam} has
become the default choice of optimizer for a large number of machine learning
applications. The algorithm keeps track of two moving averages; the average of the
gradient ($m_t$) and the squared gradient ($v_t$). Related to these two quantities
are two hyperparameters, $\beta_1, \beta_2 \in [0,1)$, which control the exponential
decay rates of these moving averages. The moments are described mathematically as
\begin{equation}
	m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t
\end{equation}
\begin{equation}
	v_t = \beta_2 v_{t-1} + (1 - \beta:2)g_t^2
\end{equation}
where $g_t$ is the gradient w.r.t the objective function at timestep $t$.
In the paper, the authors also describe a problem with the initializing the moving
averages as vectors of 0's - it leads to moment estimates that are biased towards
zero, especially during the initial timesteps, and with small decay rates ($\beta s$
close to 1). They do, however, provide a simple countermeasure to this bias, leading
to the bias-currected moment estimates given by
\begin{equation}
	\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
\end{equation}
\begin{equation}
	\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
\end{equation}
where $\beta_i^t$ reads as $\beta_i$ to the power $t$.
The final parameter update is given by
\begin{equation}
	x_{n+1} = x_n - \gamma\frac{\hat{m}_t}{\hat{v}_t + \epsilon}
\end{equation}
where $\gamma$ is the learning rate and $\epsilon$ is added to avoid divide by zero
for small gradient values. The authors propose default settings for the hyperparameters, which are
$\beta_1 = 0.9$, $\beta_2 = 0.999$, $\gamma_1 = 10^{-3}$, and $\epsilon = 10^{-8}$.
These are the values used for every model trained in this thesis, unless otherwise
is specifically indicated.

\section{Neural Networks}
Artificial neural networks (ANN) are computational systems that can learn to
perform tasks by considering examples, generally without being
programmed with any task-specific rules. It is supposed to mimic a
biological system, wherein neurons interact by sending signals in the
form of mathematical functions between layers. All layers can contain
an arbitrary number of neurons, and each connection is represented by
a weight variable.
The field of artificial neural networks has a long history of
development, and is closely connected with the advancement of computer
science and computers in general.

In natural sciences, ANNs have already found numerous applications.
In statistical physics, they have been applied to detect
phase transitions in 2D Ising and Potts models, lattice gauge
theories, and different phases of polymers, or solving the
Navier-Stokes equation in weather forecasting.  Deep learning has also
found interesting applications in quantum and nuclear physics.

The applications are not limited to the natural sciences. There is a
plethora of applications in essentially all disciplines, from the
humanities to life science and medicine.

\subsection{Artificial Neurons}
A model of artificial neurons was first developed by McCulloch and Pitts in 1943 
\cite{McCulloch1943} to study signal
processing in the brain and has later been refined by others. The
general idea is to mimic neural networks in the human brain, which is
composed of billions of neurons that communicate with each other by
sending electrical signals.  Each neuron accumulates its incoming
signals, which must exceed an activation threshold to yield an
output. If the threshold is not overcome, the neuron remains inactive,
i.e. has zero output. This behaviour inspired a simple mathematical model for an artificial neuron.

\begin{equation}\label{eq:artificial-neuron}
 y = f\left(\sum_{i=1}^n w_ix_i\right) = f(z)
\end{equation}

Here, the output $y$ of the neuron is the value of its activation function,
which receives as input a weighted sum of signals $x_i, \dots ,x_n$ received by $n$ other neurons.
Neurons are often referred to as "nodes" or "units" in machine learning literature,
and we will use these interchangeably in the following sections.

\subsection{The Feed Forward Neural Network}
A network of only one neuron such as the one described above is typically referred
to as a \textit{perceptron}. The simplest network structure contains a
single layer of N such nodes, and is most often called a \textit{single-layer perceptron}.
Adding additional layers of nodes, so-called \textit{hidden layers}, results in a
type of feed-forward neural network (FFNN), typically referred to as a Multilayer Perceptron (MLP)
(see figure \ref{fig:theory-mlp}). The example is also a fully connected network, as every
node in a layer is connected to every node in the next. The name "feed-forward" stems from
the fact that information flows in only one direction: forward through the layers.
\input{chapters/theory/figures/neural_network_example.tex}
First, for each node $i$ in the first hidden layer, we calculate a weighted sum $z_i^1$ of the input coordinates $x_j$,

\begin{equation} 
	z_i^1 = \sum_{j=1}^{M} w_{ij}^1 x_j + b_i^1
\end{equation}
Here $b_i$ is the \textit{bias} which is needed in
case of zero activation weights or inputs. How to fix the biases and
the weights will be discussed below.  The value of $z_i^1$ is the
argument to the activation function $f_i$ of each node $i$, The
variable $M$ stands for all possible inputs to a given node $i$ in the
first layer.  We define  the output $y_i^1$ of all neurons in layer 1 as

\begin{equation}\label{eq:output-layer}
	y_i^1 = f(z_i^1) = f\left(\sum_{j=1}^M w_{ij}^1 x_j  + b_i^1\right)
\end{equation}
where we assume that all nodes in the same layer have identical
activation functions, hence the notation $f$. In general, we could assume in the more general case that different layers have different activation functions.
In this case we would identify these functions with a superscript $l$ for the $l$-th layer,

\begin{equation}\label{eq:general-layer}
	y_i^l = f^l(u_i^l) = f^l\left(\sum_{j=1}^{N_{l-1}} w_{ij}^l y_j^{l-1} + b_i^l\right)
\end{equation}
where $N_l$ is the number of nodes in layer $l$. When the output of
all the nodes in the first hidden layer are computed, the values of
the subsequent layer can be calculated and so forth until the output
is obtained.

The output of neuron $i$ in layer 2 is thus,

\begin{align}\label{eq:output-layer-2}
	y_i^2 &= f^2\left(\sum_{j=1}^N w_{ij}^2 y_j^1 + b_i^2\right) \\
	&= f^2\left[\sum_{j=1}^N w_{ij}^2f^1\left(\sum_{k=1}^M w_{jk}^1 x_k + b_j^1\right) + b_i^2\right]
\end{align}
where we have substituted $y_k^1$ with the inputs $x_k$. Finally, the ANN output reads

\begin{align}
	y_i^3 &= f^3\left(\sum_{j=1}^N w_{ij}^3 y_j^2 + b_i^3\right) \\
	&= f_3\left[\sum_{j} w_{ij}^3 f^2\left(\sum_{k} w_{jk}^2 f^1\left(\sum_{m} w_{km}^1 x_m + b_k^1\right) + b_j^2\right)
	+ b_1^3\right]
\end{align}

We can generalize this expression to an MLP with $l$ hidden
layers. The complete functional form is,

\begin{align}\label{eq:complete-nn}
&y^{l+1}_i = f^{l+1}\left[\!\sum_{j=1}^{N_l} w_{ij}^3 f^l\left(\sum_{k=1}^{N_{l-1}}w_{jk}^{l-1}\left(\dots f^1\left(\sum_{n=1}^{N_0} w_{mn}^1 x_n+ b_m^1\right)\dots\right)+b_k^2\right)+b_1^3\right] &&
\end{align}

which illustrates a basic property of MLPs: The only independent
variables are the input values $x_n$.
This confirms that an MLP, despite its quite convoluted
mathematical form, is nothing more than an analytic function, specifically a
mapping of real-valued vectors $\bs{x} \in \mathbb{R}^n \rightarrow
\bs{y} \in \mathbb{R}^m$.

Furthermore, the flexibility and universality of an MLP can be
illustrated by realizing that the expression is essentially a nested
sum of scaled activation functions of the form
\begin{equation}
 f(x) = c_1 f(c_2 x + c_3) + c_4
\end{equation}

where the parameters $c_i$ are weights and biases. By adjusting these
parameters, the activation functions can be shifted up and down or
left and right, change slope or be rescaled which is the key to the
flexibility of a neural network.

We will now introduce a more convenient notation for the activations in an ANN. 
We can represent the biases and activations
as layer-wise column vectors $\bs{b}_l$ and $\bs{y}_l$, so that the $i$-th element of each vector 
is the bias $b_i^l$ and activation $y_i^l$ of node $i$ in layer $l$ respectively. 

We have that $\bs{W}_l$ is an $N_{l-1} \times N_l$ matrix, 
while $\bs{b}_l$ and $\bs{y}_l$ are $N_l \times 1$ column vectors. 
With this notation, the sum becomes a matrix-vector multiplication, and we can write
the equation for the activations of hidden layer 2 (assuming three nodes for simplicity) as
\begin{equation}
 \bs{y}_2 = f_2(\bs{W}_2 \bs{y}_{1} + \bs{b}_{2}) = 
 f_2\left(\left[\begin{array}{ccc}
    w^2_{11} &w^2_{12} &w^2_{13} \\
    w^2_{21} &w^2_{22} &w^2_{23} \\
    w^2_{31} &w^2_{32} &w^2_{33} \\
    \end{array} \right] \cdot
    \left[\begin{array}{c}
           y^1_1 \\
           y^1_2 \\
           y^1_3 \\
          \end{array}\right] + 
    \left[\begin{array}{c}
           b^2_1 \\
           b^2_2 \\
           b^2_3 \\
          \end{array}\right]\right).
\end{equation}

The activation of node $i$ in layer 2 is
\begin{equation}
 y^2_i = f_2\Bigr(w^2_{i1}y^1_1 + w^2_{i2}y^1_2 + w^2_{i3}y^1_3 + b^2_i\Bigr) = 
 f_2\left(\sum_{j=1}^3 w^2_{ij} y_j^1 + b^2_i\right).
\end{equation}

This is not just a convenient and compact notation, but also a useful
and intuitive way to think about MLPs: The output is calculated by a
series of matrix-vector multiplications and vector additions that are
used as input to the activation functions. For each operation
$\bs{W}_l \bs{y}_{l-1}$ we move forward one layer.

\subsection{Activation Functions}
Other than its connectivity, the choice of which activation function(s) to employ 
is one of the defining properties of a neural network. Not just any function will do,
however, and there are several restrictions imposed on any applicable function.
An activation function for an FFNN must be
\begin{itemize}
	\item Non-constant
	\item Bounded
	\item Monotonically-increasing
	\item Continuous
\end{itemize}

\noindent As linear functions are not bounded, the second requirement exludes this entire family of functions.
The output of a neural network with linear activation functions would be nothing more than a linear function
of the inputs. We need to introduce some form of non-linearity to be able to fit non-linear functions.
The most common examples of such functions are the logistic \textit{sigmoid}
\begin{equation}\label{eq:sigmoid}
	\sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}
and the \textit{hyperbolic tangent}
\begin{equation}\label{eq:hyp-tangent}
	f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}.
\end{equation}

\noindent In addition to meeting the requirements, these functions also have derivatives that are relatively cheap
to compute. The sigmoid's derivative is
\begin{equation}
	\ddx{\sigma(x)} = \sigma(x)(1-\sigma(x)),
\end{equation}
and the hyperbolic tangents is
\begin{equation}
	\ddx{tanh(x)} = 1 - tanh^2(x)
\end{equation}
However, the sigmoid functions suffer from saturating gradients. This occurs when the functions value
changes little to nothing with changes in the value of x. This has lead to a continuous search for
alternatives, and one of the most popular activation functions to day is the \textit{Rectified Linear Unit} (ReLU).
The function, made especially popular after the success of Krizhevsky et al. \cite{Krizhevsky2012},
takes the following form
\begin{equation}\label{eq:relu}
	\text{ReLU} (x) = f(x) = \begin{cases}
	x, & \text{if } x > 0 \\
	0,  & \text{otherwise} .
	\end{cases}
\end{equation}

\noindent This function is certainly monotonic, and we can approximate its derivative with the Heaviside
step-function, denoted $H(x)$ on the following form
\begin{equation}\label{eq:heaviside}
	H(x) = f'(x) = 	\begin{cases}1, & \text{if } x > 0 \\
		0,  & \text{otherwise}.
	\end{cases}
\end{equation}

\subsection{Backpropagation}

As we have seen now in a feed-forward network, we can express the final output of our 
network in terms of basic matrix-vector multiplications. The unknown quantities are 
our weights $w_{ij}$ and we need to find an algorithm for changing them so that our 
errors are as small as possible. This leads us to the famous backpropagation algorithm
\cite{Rumelhart1986}.

The questions we want to ask are how do changes in the biases and the
weights in our network change the cost function and how can we use the
final output to modify the weights?
To derive these equations let us start with a plain regression problem
and define our cost function 

$$\mathcal{C}(\bs{W})  =  \frac{1}{2}\sum_{i=1}^n\left(\hat{y_i} - y_i\right)^2,$$

where the $y_i$'s are our $n$ targets (the values we want to
reproduce), while the outputs of the network after having propagated
all inputs $\hat{x}$ are given by $y_i$.  Below we will demonstrate
how the basic equations arising from the backpropagation algorithm
can be modified to study classification problems with $K$ classes.

With our definition of the targets $\bs{y}$, the outputs of the
network $\hat{\bs{y}}$ and the inputs $\bs{x}$ we
define now the activation $z_j^l$ of node $j$ of the
$l$-th layer as a function of the bias, the weights which add up from
the previous layer $l-1$ and the outputs
$\bs{a}^{l-1}$ from the previous layer as

$$z_j^l = \sum_{i=1}^{M_{l-1}}w_{ij}^la_i^{l-1}+b_j^l,$$

where $b_k^l$ are the biases from layer $l$.  Here $M_{l-1}$
represents the total number of nodes of layer $l-1$. 
We can rewrite this in a more compact form as the matrix-vector products 
we discussed earlier,

$$\bs{z}^l = \left(\bs{W}^l\right)^T\bs{a}^{l-1}+\bs{b}^l.$$

With the activation values $\bs{z}^l$ we can in turn define the
output of layer $l$ as $\bs{a}^l = f(\bs{z}^l)$ where $f$ is our
activation function. In the examples here we will use the sigmoid
function discussed in our logistic regression lectures. We will also use the same activation function $f$ for all layers
and their nodes.  It means we have

$$a_j^l = f(z_j^l) = \frac{1}{1+e^{-(z_j^l)}}.$$

\noindent From the definition of the activation $z_j^l$ we have

$$\frac{\partial z_j^l}{\partial w_{ij}^l} = a_i^{l-1},$$
and
$$\frac{\partial z_j^l}{\partial a_i^{l-1}} = w_{ji}^l.$$

\noindent With our definition of the activation function we have that (note that this function depends only on $z_j^l$)

$$\frac{\partial a_j^l}{\partial z_j^{l}} = a_j^l(1-a_j^l)=f(z_j^l)(1-f(z_j^l)).$$

\noindent With these definitions we can now compute the derivative of the cost function in terms of the weights.
Let us specialize to the output layer $l=L$. Our cost function is
$$\mathcal{C}(\bs{W}^L)  =  \frac{1}{2}\sum_{i=1}^n\left(\hat{y}_i - y_i\right)^2=\frac{1}{2}\sum_{i=1}^n\left(a_i^L - y_i\right)^2,$$
The derivative of this function with respect to the weights is
$$\frac{\partial \mathcal{C}(\bs{W}^L)}{\partial w_{jk}^L}  =  \left(a_j^L - t_j\right)\frac{\partial a_j^L}{\partial w_{jk}^{L}},$$
The last partial derivative can easily be computed and reads (by applying the chain rule)
$$\frac{\partial a_j^L}{\partial w_{jk}^{L}} = \frac{\partial a_j^L}{\partial z_{j}^{L}}\frac{\partial z_j^L}{\partial w_{jk}^{L}}=a_j^L(1-a_j^L)a_k^{L-1},$$
We have thus
$$\frac{\partial \mathcal{C}(\bs{W}^L)}{\partial w_{jk}^L}  =  \left(a_j^L - t_j\right)a_j^L(1-a_j^L)a_k^{L-1},$$

\noindent Defining

$$\delta_j^L = a_j^L(1-a_j^L)\left(a_j^L - t_j\right) = f'(z_j^L)\frac{\partial {\cal C}}{\partial (a_j^L)},$$

and using the Hadamard product of two vectors we can write this as

\begin{equation}\label{eq:bprop-error}
	\bs{\delta}^L = f'(\bs{z}^L)\circ\frac{\partial \mathcal{C}}{\partial (\bs{a}^L)}.
\end{equation}

\noindent This is an important expression. The second term on the right-hand side
measures how fast the cost function is changing as a function of the $j$th
output activation.  If, for example, the cost function doesn't depend
much on a particular output node $j$, then $\delta_j^L$ will be small,
which is what we would expect. The first term on the right measures
how fast the activation function $f$ is changing at a given activation
value $z_j^L$.

\noindent Notice that everything in the above equations is easily computed.  In
particular, we compute $z_j^L$ while computing the behaviour of the
network, and it is only a small additional overhead to compute
$f'(z^L_j)$.  The exact form of the derivative with respect to the
output depends on the form of the cost function.
However, provided the cost function is known there should be little
trouble in calculating

$$\frac{\partial \mathcal{C}}{\partial (a_j^L)}$$

\noindent With the definition of $\delta_j^L$ we have a more compact definition of the derivative of the cost function in terms of the weights, namely
\begin{equation}\label{eq:cost-derivative-compact}
	\frac{\partial{\cal C}(\hat{W^L})}{\partial w_{jk}^L}  =  \delta_j^La_k^{L-1}.
\end{equation}
It is now possible to rewrite our previous equation for $\delta_j^L$ (\ref{eq:bprop-error}) as
\begin{equation}
	\delta_j^L =\frac{\partial \mathcal{C}}{\partial z_j^L}= \frac{\partial \mathcal{C}}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L},
\end{equation}
which can also be interpreted as the partial derivative of the cost function with respect to the biases $b_j^L$, namely

$$\delta_j^L = \frac{\partial\mathcal{C}}{\partial b_j^L}\frac{\partial b_j^L}{\partial z_j^L}=\frac{\partial\mathcal{C}}{\partial b_j^L},$$
That is, the error $\delta_j^L$ is exactly equal to the rate of change of the cost function as a function of the bias.

We now have three equations that are essential for the computations of the derivatives of the cost function at the output layer.
These equations are needed to start the algorithm and they are

\begin{equation}
\frac{\partial\mathcal{C}(\bs{W}^L)}{\partial w_{jk}^L}  =  \delta_j^La_k^{L-1},
\end{equation}
and
\begin{equation}
\delta_j^L = f'(z_j^L)\frac{\partial {\cal C}}{\partial a_j^L},
\end{equation}
and
\begin{equation}
\delta_j^L = \frac{\partial\mathcal{C}}{\partial b_j^L},
\end{equation}

\noindent A consequence of the above equations is that when the
activation $a_k^{L-1}$ is small, the gradient term, that is the
derivative of the cost function with respect to the weights, will also
tend to be small. From this we gather that the weight changes (or "learns") slowly when we
minimize the weights via gradient descent. 

Another feature is that when the activation function (in this case sigmoid),
is rather flat when we move towards its limit values $0$ and $1$. In these
cases, the derivatives of the activation function will also be close
to zero, meaning again that the gradients will be small and the
network learns slowly again.

\noindent We need a fourth equation and we are set. We are going to propagate
backwards to determine the weights and biases. To do so we need to represent
the error in the layer before the final one $L-1$ in terms of the errors in the 
final output layer. Replacing the final layer $L$ with a general layer $l$, we have
$$\delta_j^l =\frac{\partial\mathcal{C}}{\partial z_j^l}.$$
We want to express this in terms of the equations for layer $l+1$.
Using the chain rule and summing over all $k$ entries we have

$$\delta_j^l =\sum_k \frac{\partial\mathcal{C}}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^{l}}=\sum_k \delta_k^{l+1}\frac{\partial z_k^{l+1}}{\partial z_j^{l}},$$
and recalling that
$$z_j^{l+1} = \sum_{i=1}^{M_{l}}w_{ij}^{l+1}a_i^{l}+b_j^{l+1},$$
with $M_l$ being the number of nodes in layer $l$, we arrive at
\begin{equation}\label{eq:backprop-general}
	\delta_j^l =\sum_k \delta_k^{l+1}w_{kj}^{l+1}f'(z_j^l),
\end{equation}
The four equations provide us with a way of computing the gradient of the cost function.

First, we set up the input data $\bs{x}$ and the activations
$\bs{z}_1$ of the input layer and compute the activation function and
the pertinent outputs $\bs{a}^1$.

Secondly, we perform the feed-forward until we reach the output
layer and compute all $\bs{z}_l$ of the input layer and compute the
activation function and the pertinent outputs $\bs{a}^l$ for
$l=2,3,\dots,L$.
Next we compute the ouput error $\bs{\delta}^L$ by computing all
$$\delta_j^L = f'(z_j^L)\frac{\partial\mathcal{C}}{\partial a_j^L}.$$
Then we compute the back propagate error for each $l=L-1,L-2,\dots,2$ as
$$\delta_j^l = \sum_k \delta_k^{l+1}w_{kj}^{l+1}f'(z_j^l).$$
Finally, we update the weights and the biases using gradient descent for 
each $l=L-1,L-2,\dots,2$ and update the weights and biases according to the rules
$$w_{jk}^l\leftarrow  = w_{jk}^l- \eta \delta_j^la_k^{l-1},$$

$$b_j^l \leftarrow b_j^l-\eta \frac{\partial {\cal C}}{\partial b_j^l}=b_j^l-\eta \delta_j^l,$$

\noindent The parameter $\eta$ is the learning parameter discussed in connection with the 
gradient descent methods.

\subsection{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs) share quite a few similarities with
ordinary neural networks, and all the concepts developed for neural networks
so far still apply. The difference is that CNNs assume the inputs to be images.

A problem with regular neural networks is that they scale poorly to large images.
As an example, consider an image of size $32\times 32\times 3$ 
(32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first 
hidden layer of a regular Neural Network would have 
$32\times 32\times 3 = 3072$ weights. This amount still seems manageable, but
clearly this fully-connected structure does not scale to larger images. For example,
an image of more respectable size, say $200\times 200\times 3$, would lead to neurons
that have $200\times 200\times 3 = 120,000$ weights. Adding several such neurons then
quickly increases the number of parameters, which in turn increases the risk of overfitting.

\noindent CNNs take advantage of the fact that the input consists of images and they
constrain the architecture in a more sensible way. In particular, unlike a regular 
NN, the layers of a CNN have neurons arranged in 3 dimensions: width, height, depth. 
(Note that the word depth here refers to the third dimension of an activation volume,
not to the depth of a full NN, which can refer to the total number of layers in a 
network.) The above example of an image with an input volume of activations has 
dimensions $32\times 32\times 3$ (width, height, depth respectively).
See figure \ref{fig:cnn-example} for an illustration.

The neurons in a layer will only be connected to a small region of the layer before 
it, instead of all of the neurons in a fully-connected manner. Moreover, the final
output layer could  for this specific image have dimensions $1\times 1 \times 10$, 
because by the end of the CNN architecture we will reduce the full image into a
single vector of class scores, arranged along the depth dimension. 
\begin{figure}
	\includegraphics[width=\textwidth]{chapters/theory/figures/cnn.jpeg}
	\caption{\label{fig:cnn-example}A CNN arranges its neurons in three dimensions (width, height, depth), 
	as visualized in one of the layers. Every layer of a CNN transforms 
	the 3D input volume to a 3D output volume of neuron activations. 
	In this example, the red input layer holds the image, so its width 
	and height would be the dimensions of the image, and the depth would be 3 
	(Red, Green, Blue channels). Image borrowed from CS231n's github page \cite{cs231n}
	on CNNs}
\end{figure}

\noindent A simple CNN is a sequence of layers, and every layer of a CNN transforms one volume
of activations to another through a differentiable function. We use three main types 
of layers to build CNN architectures: Convolutional Layer, Pooling Layer, and
Dense (fully connected, exactly as seen in regular Neural Networks). We will stack 
these layers to form a full CNN architecture.

\noindent A simple CNN for image classification could have the architecture:
\begin{itemize}
	\item \textbf{INPUT} ($32\times 32 \times 3$) will hold the raw pixel values of
	the image, in this case an image of width 32, height 32, and with three color 
	channels R,G,B.
	\item \textbf{CONV} (convolutional )layer will compute the output of neurons 
	that are connected to local regions in the input, each computing a dot product 
	between their weights and a small region they are connected to in the input 
	volume. This may result in volume such as $[32\times 32\times 12]$ if we decided 
	to use 12 filters.
	\item \textbf{RELU} layer will apply an elementwise activation function,
	such as the $max(0,x)$ thresholding at zero. This leaves the size of the volume 
	unchanged ($[32\times 32\times 12]$).
	\item \textbf{POOL} (pooling) layer will perform a downsampling operation along
	the spatial dimensions (width, height), resulting in volume such as 
	$[16\times 16\times 12]$.
	\item \textbf{DENSE} (i.e. fully-connected) layer will compute the class scores,
	resulting in volume of size $[1\times 1\times 10]$, where each of the 10 numbers 
	correspond to a class score, such as among the 10 categories of the MNIST images 
	we considered above . As with ordinary Neural Networks and as the name implies,
	each neuron in this layer will be connected to all the numbers in the previous
	volume.
\end{itemize}

\noindent CNNs transform the original image layer by layer from the original
pixel values to the final class scores. Observe that some layers contain
parameters and other don’t. In particular, the CNN layers perform
transformations that are a function of not only the activations in the
input volume, but also of the parameters (the weights and biases of
the neurons). On the other hand, the RELU/POOL layers will implement a
fixed function. The parameters in the CONV/FC layers will be trained
with gradient descent so that the class scores that the CNN computes
are consistent with the labels in the training set for each image.

\noindent For a more in-depth breakdown of convolutional neural networks,
we refer to Stanford's excellent course CS231n, and their text on CNNs \cite{cs231n}.



\section{Assessing the Performance of Models}
If classification accuracy is not enough to gauge whether a model is
performing well, or well in the desired way, alternative way to measure
performance must be explored. For cases of imbalanced data there are a few
widely used methods that reveal information about the model that the simple
accuracy metric can't.

\subsection{Imbalanced Data in Classification}
The physical world is rarely balanced.

A common challenge in classification is imbalanced data, in which a large
amount of the labeled data belongs to just one or a few of the classes.
For binary classification, if 90\% of the data belongs to one of the classes,
then the classifier is likely to end up placing every single
input in that class, as it will bring its accuracy to 90\%. Technically, this
accuracy is correct, but it's not very useful since the decision isn't at all
affected by the features of the input. Accuracy alone isn't a good enough
measure of performance to reveal this.

\subsection{Confusion Matrix}
A confusion matrix is an n by n matrix containing correct classifications
on the diagonal, and false positives and negatives in the off-diagonal elements.
An example of such a matrix could be the following table:
\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c}
     & True Cat & True Dog & True Rabbit \\
    \hline
    Predicted Cat & \textbf{5} & 2 & 0 \\
    \hline
    Predicted Dog & 3 & \textbf{3} & 2 \\
    \hline
    Predicted Rabbit & 0 & 1 & \textbf{11} \\
\end{tabular}
\caption{Confusion matrix for an example classification where the classes
         are Cat, Dog and Rabbit. Correct classifications in bold.}
\label{tab:confmat-example}
\end{table}
In the table above (\ref{tab:confmat-example}), the diagonal elements
$i = j$ are the correct classifications, while the other elements correspond
to cases where the model predicted class $i$ but should've predicted class $j$.
The confusion matrix thus gives information about false positives and false
negatives, in addition to classification accuracy. This is very useful
in cases where for example false positives can be readily ignored or filtere
later, but false negatives may have severe consequences. An example of this
could be detection of cancer, in which a false positive can be ruled out
from further testing, while a false negative may lead to a patient being sent
home when actually needing help. For a more in-depth look at confusion matrices
see \cite{Fawcett2006}.

\subsection{Receiver Operating Characteristic}
The Receiver Operating Characteristic (ROC) is a widely used measure of a
classifiers performance . The performance is measured as the effect
of the true positive rate (TPR) and the false positive rate (FPR) as a function
of thresholding the positive class. To evaluate the ROC curve for a model,
traditionally the Area Under the Curve (AUC) is used, which ranges from 0
(an ideal "opposite" classifier) to 1.0 (an ideal classifier) with 0.5
indicating a random choice classifier,
For a thorough explanation of ROC curves and the underlying concepts, see \cite{Fawcett2006}.

\section{Nuclear Science}
\subsection{Shell Structure}
\begin{itemize}
	\item Comprehensive and predictive model of atomic nuclei
	\begin{itemize}
		\item Evolving structure of atomic nuclei as a function of protons and neutrons from first principles
	\end{itemize}
	\item Understanding the origin of the elements
	\begin{itemize}
		\item Explosive nucleosynthesis
	\end{itemize}
	\item Use of atomic nuclei to test fundamental symmetries
	\item Search for new applications of isotopes and solutions to societal problems
\end{itemize}

\subsubsection{Nomenclature}
A nucleus $Y$ has $Z$ protons and $N$ neutrons with a mass of $A = Z + N$.
This is written as \ce{^{A}_{Z}Y_{N}}. For a given nucleus there may be several
\begin{itemize}
	\item Isotopes - nuclei with the same number of protons, but varying number of neutrons
	\begin{itemize}
		\item \ce{^{66}Ni}, \ce{^{67}Ni}, \ce{^{68}Ni}, \ce{^{69}Ni}, \ce{^{70}Ni}
	\end{itemize}
	\item Isotones - nuclei with the same number of neutrons, but varying number of protons
	\begin{itemize}
		\item \ce{^{70}Zn}, \ce{^{69}Cu}, \ce{^{68}Ni}, \ce{^{67}Co}, \ce{^{66}Fe}
	\end{itemize}
	\item Isobars - nuclei with the same number of nucleons $A$
	\begin{itemize}
		\item \ce{^{68}Fe}, \ce{^{68}Co}, \ce{^{68}Ni}, \ce{^{68}Cu}, \ce{^{68}Zn}
	\end{itemize}
\end{itemize}