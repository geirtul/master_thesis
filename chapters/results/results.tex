The machine learning experiments conducted in this thesis were performed
using the AI-Hub computational cluster at the University of Oslo. This resource 
consists of three machines with four RTX 2080 Nvidia GPUâ€™s (graphics
processing unit) each. These cards have ~10GB of memory available for the
allocation of models.

\section{Preliminary analysis and comparison of simulated and experimental data}
\input{chapters/results/figures/preliminary_comparison_intensity.tex}
\input{chapters/results/figures/preliminary_intensity_hip_comparison.tex}
\section{Classification}
For classification we have used five different model types,
spanning a range of complexities, architectures, and parameters.
The reason for multiple models is the fundamental difference between the
simulated and experimental datasets; labels. Without ground truth labels
for the experimental data, we need other ways to assess how a model performs in
its task. We attempt to gain some insight by comparing the models' performance on
simulated data with their outputs when applied to experimental data.

\noindent We train the models on three sets of simulated data.
The first (a) is the simulated data as-is, with no changes apart from normalization
and pre-processing. The second (b) is the same data as (a), but with specific pixels in
the 'detector images' set to zero. These pixels are effectively "dead" in the
experimental dataset, and we wish to monitor the effect this has on model performance.
The last dataset (c) has been intentionally imbalanced to further mimick experimental data,
wherein we expect the majority of events to be 'single' events, rather than 'double'
events, as outlined in section \ref{sec:experimental-background}.

\noindent The variability in results is estimated using a K-fold cross-validation approach, with
$K = 5$ \cite{Stone1974}.

\subsection{Classification on simulated data}
As the data consists of detector \textit{images} we expect convolutional
neural networks to be particularly well-suited for the classification task.
In table \ref{tab:classification-simulated-all-f1-auc} the performance of each model
is reported through the estimated $F1$-score, for each of the datasets. 
\input{chapters/results/figures/classification_simulated_all_f1_auc.tex}
As a benchmark, we are including a model based on a state of the art pretrained 
network\cite{Simonyan2015}, as outlined in section \ref{sec:method-pretrained}. 
In in figure \ref{fig:confmat-simulated}, we show the confusion matrix for prediction
on test set data for all the models, including normalized values for each event type.
\input{chapters/results/figures/confmat-simulated.tex}. The F1-scores show decreasing
performance for most models when applying modifications to the original simulated dataset.
For the CNN the effect is more severe. When training on an intentionally imbalanced
dataset the models without CNN architechtures show a significant decrease in performance.
Considering the confusion matrix we see that the logistic regressor and dense network
suffer from predicting mostly every sample to be single events. Looking back to the
$F1$-scores again, there is not a significant increase in performance between the
CNN and the Custom model on unmodified data, but the custom model performs strongly
across all datasets, with a lowY amount of misclassified events relative to the other
models.

\subsection{Classification on experimental data}
Classification of experimental data poses a different set of challenges when it comes
to evaluating our results. We only have a handful of events that are handlabeled as
double events, which can be used as a form of verification. As mentioned in section
\ref{sec:experimental-background}, we expect the number of double events in the experimental
data to be much lower than single events. Inspecting the ratio of predicted singles to predicted
doubles can then be an initial indication of how a model is performing. It is, however,
not conclusive. Correctly classified handlabeled events are another indication, but
is also not conclusive. In table \ref{tab:classification-experimental-ratios}, the ratios
of predicted singles to predicted doubles are presented for each model trained on each
dataset. The actual number of predictions for each class are included below the ratios.

\input{chapters/results/figures/classification_experimental_ratios.tex}
\input{chapters/results/figures/classification_experimental_labeled_doubles.tex}
\input{chapters/results/figures/classification_experimental_single_fractions.tex}
\input{chapters/results/figures/classification_simulated_scaled.tex}
 
\section{Regression}
\subsection{Regression on simulated data}
On data already classified, we attempt to predict the energy of the events and the 
position of origin for the electrons. Because there is a travel distance between the 
ejection site and the point the energy is deposited, the positions aren't necessarily
the locations of the highest-intensity pixels in the detector images.
Note that for regression the models are trained on single or double events exclusively.
This means that for dataset $(c)$, which is imbalanced, the set of single events is
identical to that of set $(b)$, causing near-identical results between these two sets.
Due to the poor performance of models trained on simulated double events, and the
expected low frequency of double events in the experimental data, we only apply models
trained on simulated single events.
\subsubsection{Position of origin}
In table \ref{tab:regression-simulated-all-positions-r2} the R2-scores for all models
trained on simulated data are presented. A similar trend as was seen for classification
can be observed for single events, where models are trained on unmodified data. Even
a linear regressor is able to predict positions of origin fairly well, but it is clear
that the neural networks perform at a much higher level in this case. They have strong
R2-scores of 0.99 and higher, and very little degradation in performance with added 
modifications to the training data. Again, as with classification, the custom model
performs strongly across all datasets for single events.
In the case of regression on double events, none of the models are able to accurately
predict positions of origins for both events.
\input{chapters/results/figures/regression_simulated_all_positions_r2.tex}
\subsubsection{Energy}
In table \ref{tab:regression-simulated-all-energies-r2} the R2-scores for all models
trained on simulated data are presented.
Performance when predicting single energies is across the board lower when compared with
R2-scores for positions. On unmodified data the models are to a large degree able to
predict energies, with R2-score of 0.93 and above. For the modified datasets the CNN
suffers greatly and isn't able to predict energies at all. Other models see a less
severe effect, but still a clear reduction in performance.
As was the case with prediction of positions, no models predict energies
of double events with any useful degree of accuracy.
\input{chapters/results/figures/regression_simulated_all_energies_r2.tex}

\subsection{Regression on experimental data}
\input{chapters/results/figures/regression-experimental-best-model-dist.tex}
\input{chapters/results/figures/regression_experimental_dist_means.tex}