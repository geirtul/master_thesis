\noindent The first task we face is whether or not we can use machine learning algorithms to 
separate two types of events in a simulated nuclear physics dataset. This being possible
is the minimum requirement a model must meet if we are to apply it to experimental data.
We present the results of training five different models on three simulated 
datasets, as described in section \ref{sec:experimental-background-data}).
Performance is measured using the $F1$ score and confusion matrix. The models range from
the simple logistic regressor, to a deeper CNN architecture with multiple layers. Additionally,
we include a model based on VGG16, as outlined in section \ref{section:method-pretrained},
fine-tuned on simulated data. Somewhat separate from classification, our second objective
is predicting positions of origin and energies associated with events in the dataset.
We follow the same strategy as for classification, with the logistic model replaced by a linear
regressor. The same minimum requirement is valid for regression - models trained on simulated data
must be able to make reasonable predictions. By 'reasonable' we mean 'better than random guessing'.
For this task the performance is measured using the $R2$ score. The variability in results is 
estimated using a K-fold cross-validation approach, with $K = 5$ \cite{Stone1974}.
As a quick recap, the three simulated datasets mentioned are:
\begin{enumerate}[a)]
    \item No changes.
    \item Select pixels set to zero throughout the dataset.
    \item Select pixels set to zero throughout the data, and imbalanced number of single
    and double decays (reduced amount of double decays).
\end{enumerate}

\noindent The machine learning experiments conducted in this thesis were performed
using the AI-Hub computational cluster at the University of Oslo. This resource 
consists of three machines with four RTX 2080 Nvidia GPUâ€™s (graphics
processing unit) each. These cards have ~10GB of memory available for the
allocation of models.

\section{Preliminary analysis}
We begin by looking into the simulated data, more specifically looking for correlations
in the energies and pixel intensities. Note that these results are generated using
dataset (a). In figure \ref{fig:simulated_corrcoeff} we show
the correlation matrix for simulated single and double decays. For single decays, there
is a strong correlation between the energy of the event, the sum of intensities in an image,
and also between the event energy and the intensity of the highest intensity pixel.
The correlation matrix for double events shows similar results. The same, strong correlation
is found for $E1 + E2$ in double decays as for $E1$ in single decays. That is, the total energy
in an event is strongly correlated with the total intensity of pixels in the image.
\input{chapters/results/figures/preliminary_simulated_corrcoeff.tex}

\noindent Next, we investigate some directly comparable quantities shared by simulated and
experimental data. The distributions of total intensity and highest intensity pixels
in images are shown in figure \ref{fig:comparison-intensity}. 
\input{chapters/results/figures/preliminary_comparison_intensity.tex}
\noindent To relate these distributions more closely to the models themselves, 
the distributions are generated after normalization of the
images. As such, these distributions are what the models 'see'. Looking to the top left
in figure \ref{fig:comparison-intensity} there are a fairly large amount of experimental
decays with higher total intensity than what is present in the simulated data. In the bottom
left plot, it is also clear that there is a point where simulated single and double decays no longer
overlap in total intensities. For the right-hand plots of highest intensity values, there is no such
clear difference between the datasets.
To provide another point of view for this difference, we plot the total intensity in images as
a function of the highest intensity in the images. 
The plot is shown in figure \ref{fig:intensity-hip-comparison}, along with linear fits to the
data.
\input{chapters/results/figures/preliminary_intensity_hip_comparison.tex}
\noindent There is a clear difference between the datasets in that the experimental data
has a higher total intensity. We will come back to these results as we review model
performance on experimental data.

\section{Classification}
\subsection{Classification on simulated data}
In table \ref{tab:classification-simulated-all-f1-auc} the performance of each model
is reported through the estimated $F1$-score, for each of the datasets. 
\input{chapters/results/figures/classification_simulated_all_f1_auc.tex}
As a benchmark for transfer learning, we are including a model based on a state of the art pretrained 
network\cite{Simonyan2015}, as outlined in section \ref{sec:method-pretrained}. 
In in figure \ref{fig:confmat-simulated}, we show the confusion matrix for prediction
on test set data for all the models, including normalized values for each event type.
\input{chapters/results/figures/confmat-simulated.tex}. The F1-scores show decreasing
performance for most models when applying modifications to the unmodified simulated dataset.
When training on an intentionally imbalanced dataset the models without CNN architectures 
show a steep decrease in performance.
Considering the confusion matrix we see that the logistic regressor and dense network
suffer from predicting mostly every sample to be single events. Looking back to the
$F1$-scores again, there is not a significant increase in performance between the
CNN and the Custom model on unmodified data, but the custom model performs strongly
across all datasets, with a low amount of misclassified events relative to the other
models. Again looking at the confusion matrices (figure \ref{fig:confmat-simulated}),
Double decays are more often misclassified as single than the opposite.
Next, we apply the 'classifiers' to experimental data.

\subsection{Classification on experimental data}
Classification of experimental data poses a different set of challenges when it comes
to evaluating our results. We currently only have a small number of events that are
hand labelled as double events, which may be used as a form of verification. As mentioned in section
\ref{sec:experimental-background}, we expect the number of double events in the experimental
data to be much lower than single events. Inspecting the ratio of predicted singles to predicted
doubles can then be an initial indication of how a model is performing. It is, however,
not conclusive. Correctly classified hand labelled events are another indication, but
is also not conclusive. In table \ref{tab:classification-experimental-ratios}, the ratios
of predicted singles to predicted doubles are presented for each model trained on each
dataset. The actual number of predictions for each class are included below the ratios.
\input{chapters/results/figures/classification_experimental_ratios.tex}
Overall there is a strong preference for classifying events as double decays. This makes
the validation using hand labelled doubles in table \ref{tab:classification-experimental-labeled-doubles}
somewhat moot, since it is hard to attribute these 'correct' classifications to the models'
ability to recognize the double decays.
\input{chapters/results/figures/classification_experimental_labeled_doubles.tex}
\input{chapters/results/figures/classification_experimental_single_fractions.tex}
As the number of events is large, manual inspection of each predicted class is not feasible.
In figure \ref{fig:experimental-single-fractions} we show the fraction of predicted single events
in experimental data as a function of total intensity in each image. Regardless of which simulated
dataset a model is trained on, the majority of  single events are predicted at low total intensites.
The only exception to this is the Dense model trained on dataset $c$. Keep in mind, however, that the
total intensities for experimental data span from 0 to ~18 (see figure \ref{fig:comparison-intensity}).
To further prod this apparent trend, we perform a simple test using the Custom model trained on dataset $b$.
For a set of single events for which we know the model has good performance, we multiply the image intensities
with a scaling factor from 0-10. The aim is to see the effect of increasing total intensity in
images on the classification accuracy. The result is shown in figure \ref{fig:simulated-scaled-intensities}.
\input{chapters/results/figures/classification_simulated_scaled.tex}
From this figure, there is a clear trend towards a lower fraction of events correctly classified as single 
decays when the intensities in images increases. Note that at a scaling factor of 4 the total intensities in
the simulated images approach the highest total intensities in the experimental decay data.
 
\section{Regression}
Assuming data has already been classified, we aim to predict the energy of the decays and their
position of origin. Because there is a travel distance between the 
ejection site and the point the energy is deposited, the positions aren't necessarily
the locations of the highest-intensity pixels in the detector images. Our hypothesis is
then that there are sufficient structures and spacial relationships in the detector images to
allow a model to determine these positions.
Note that for regression the models are trained on single or double events exclusively.
A consequence of this is that for dataset $c$ the set of single events is
identical to that of set $b$, causing near-identical results between these two sets.

\subsection{Position of origin}
In table \ref{tab:regression-simulated-all-positions-r2} the R2-scores for all models
trained on simulated data are presented.
\input{chapters/results/figures/regression_simulated_all_positions_r2.tex}
\noindent A similar trend as was seen for classification
can be observed for single events, where models are trained on unmodified data. Even
a linear regressor predicts positions of origin fairly well, but it is clear
that the neural networks perform at a much higher level in this case. They have strong
R2-scores of 0.99 and higher, and very little degradation in performance with added 
modifications to the training data. Again, as with classification, the custom model
performs strongly across all datasets for single events, with the lowest variability across
experiments. However, in this regression task, the pretrained model.
In the case of regression on double events, none of the models accurately
predict positions of origins for both events.
\input{chapters/results/figures/regression_experimental_dist_means.tex}
In the absence of true positions for experimental decays, we rely on other
ways to estimate how regression models perform.
\input{chapters/results/figures/regression-experimental-best-model-dist.tex}
In figure \ref{fig:experimental-pos-dist} we look at how the predicted positions
are distributed around the highest intensity pixel in each experimental decay event.
As a comparison, we plot the same distribution using true values for positions in
simulated single decay events. We can see that the distributions overlap quite well
up until their respective peaks. Predictions on experimental decays have a wider
distribution than the target data, but as we've seen in the classification results this
difference in distributions is not a unique case.

\subsubsection{Energy}
In table \ref{tab:regression-simulated-all-energies-r2} we show the R2-scores for all
models trained on simulated data.
Performance when predicting single energies is across the board lower than what we saw
for positions of origin. On unmodified data, the models are to a large degree able to
predict energies, with R2-scores of 0.93 and above. For the modified datasets the CNN
suffers greatly and isn't able to account for variances in the data at all. Other models
see a less severe effect, but still a clear reduction in the goodness of fit.
As was the case with the prediction of positions, no models predict energies
of double events with any useful degree of accuracy.
\input{chapters/results/figures/regression_simulated_all_energies_r2.tex}

Due to the poor performance of models trained on simulated double events, and the
expected low frequency of double events in the experimental data, we only apply models
trained on simulated single events. From figure \ref{fig:simulated_corrcoeff}, we know
that total image intensity and energy are closely correlated for simulated data,
and from $R2$ scores shown in table \ref{tab:regression-simulated-all-energies-r2}
that goodness of fit decreases with added 'dead' pixels. In abscence of true value for
energies in experimental data, we show in figure \ref{fig:regression-energy-intensity-prediction-energy-zeros}
the related quantities for experimental decays. Total image intensity decreases with
increasing number of pixels with value zero, as expected. The predicted energy
follows this same trend very closely. The 'peaks' in the plots correspond to events
where there are a fairly large amount of 'dead' pixels, but the decay was registered
in a region of the detector with few such pixels.
\input{chapters/results/figures/regression_total_intensity_predicted_energy_zeros.tex}
We also calculate the correlation coefficients between total image intensity and
predicted energy for all models applied to experimental data. For all models the
correlation coefficients range from 0.95 to 0.999.
