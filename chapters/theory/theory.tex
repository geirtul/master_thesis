\section{Linear Regression}\label{sec:linreg}
Suppose you have a data set consisting of $n$ data points. Each point is associated
with a scalar target $y_i$, and a vector $\hat{x}$ containing values for $p$ input
features. Assuming the target variable $y_i$ is linear in the inputs, it can be
written as a linear function of the features, given by
\begin{equation}\label{eq:ols-target}
y_i = \beta_0 x_{i,0} + \beta_1 x_{i,1} + ... + \beta_{p-1} x_{i, p-1} + \varepsilon_i,
\end{equation}
where $\beta = (\beta_0, \beta_1, \dots, \beta_{p-1})^T$ is a vector of length
$p$ containing unknown values, and $\varepsilon$ are the errors in our estimate.
This gives us a system of linear equations, which can be written in matrix form as
\begin{equation}\label{eq:ols-target-mat}
  \hat{y} = \boldsymbol{X\beta} + \hat{\varepsilon},
\end{equation}
where
\begin{equation}
\vec{X} = \left[
\begin{matrix}
x_{0,0} & x_{0,1} & x_{0,2} & ... & x_{0,p-1}\\
x_{1,0} & x_{1,1} & x_{1,2} & ... & x_{1,p-1}\\
x_{2,0} & x_{2,1} & x_{2,2} & ... & x_{2,p-1}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n-1,0} & x_{n-1,1} & x_{n-1,2} & ... & x_{n-1,p-1}
\end{matrix}
\right]
\end{equation}
The unknown values $\boldsymbol{\beta}$ are commonly referred to as \textit{weights}, denoted $\boldsymbol{w}$,
in machine learning literature. We will adopt this notation going forward.
To find the best possible weights $\boldsymbol{w}$ we want a suitable
quantity to optimize - a \textbf{cost function}, $\mathcal{C}$ (also referred to as an
\textbf{objective function}). An example of such a function is the squared error - or the Euclidian vector norm, defined as
\begin{equation}
	L_2(\boldsymbol{x}) =||\boldsymbol{x}||_2 = \left(\sum x_i^2\right)^{\frac{1}{2}}.
\end{equation}
From this we define the cost function
\begin{equation}
	\mathcal{C} = || \boldsymbol{\hat{y}} - \boldsymbol{y} ||_2 ^2.
\end{equation}
In machine learning, it is most common to cast the optimization as a minimization problem
("minimize the cost"). To find the minimum of the cost function as defined above, we need
a differentiation. To simplify that process, we rewrite the cost function on matrix form
\begin{align*}
\mathcal{C} &= || \boldsymbol{\hat{y}} - \boldsymbol{y} ||_2 ^2, \\
\mathcal{C} &= ( \boldsymbol{\hat{y}} - \boldsymbol{Xw})^T( \boldsymbol{\hat{y}} - \boldsymbol{Xw}).
\end{align*}
To minimize we take the derivative with respect to the weights $\boldsymbol{w}$,
and find the minima by setting the derivative equal to zero
\begin{align}
\nabla _{\boldsymbol{w}}\mathcal{C} &= \nabla _{\boldsymbol{w}} ( \boldsymbol{\hat{y}} - \boldsymbol{Xw})^T( \boldsymbol{\hat{y}} - \boldsymbol{Xw}), \\
&= -2\boldsymbol{X}^T\boldsymbol{\hat{y}} + 2\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{w}, \\
\boldsymbol{0} &= -2\boldsymbol{X}^T\boldsymbol{\hat{y}} + 2\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{w}, \\
\boldsymbol{X}^T\boldsymbol{\hat{y}} &= \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{w}, \\
\boldsymbol{w} &=(\boldsymbol{X}^T\boldsymbol{X})^{-1} \boldsymbol{X}^T\boldsymbol{\hat{y}}. \label{eq:ols}
\end{align}
We then assume that the matrix \(\vec{X}^T\vec{X}\)
is invertible to get the solution \cite{James2000}.

\section{Over- and underfitting}\label{sec:overfitting}
In machine learning, when fitting a model to a data set the goal is nearly always to predict 
values or classify samples from regions of data the model has not seen. This is not a simple task,
especially taking into consideration that data is rarely, if ever, noiseless. When extrapolating to 
unseen regions we must take steps to ensure the model complexity is appropriate - we want it to fit 
the signal, not the noise. First off - what do the terms "overfit" and "underfit" mean?
An overfit model will typically perform well during the fitting procedure, but when presented with
data outside the fitted region its performance decreases considerably.
An underfit model lacks the expressive power to capture core signal variations in the data.



In statistics, overfitting is "the production of an analysis that corresponds too closely or exactly to a 
particular set of data, and may therefore fail to fit additional data or predict future observations reliably".
(cite oxford dictionary). Mehta et. al \cite{Mehta2019} demonstrates this concept very well
through polynomial regression.

A step in this process is splitting the available data in two - training data and test data. We fit, or 'train'
the model on the training data, and then assess the performance of the model on the test data. This allows us
to combat the problem of model complexity. If the model is too simple, it may fail to express the global trends 
in the data - it is underfitting. More complex models may be able to capture the global trends, but run the risk of 
also capturing spurious correlations in the data, e.g noise - the model is overfitting.


\section{Regularization}\label{section:regularization}

With the computing resources available today, increasing model complexity to deal with underfitting is
usually a simple task. However, this computational freedom has led to overfitting being the common challenge
to overcome.


%OLS will not give a solution if the design matrix \(\hat{X}\) is singular
%or near singular, as it depends on \(\hat{X}^T\hat{X}\) being invertible. (cite?)
%
%Ridge regression is known as a shrinkage method. It shrinks the regression
%coefficients \(\hat{\beta}\) by imposing a penalty on their size (cite?). We are in other words adding a diagonal component
%to the matrix to invert. From OLS we therefore make the following change
%
%\begin{equation}
%	\hat{X}^T\hat{X} \rightarrow \hat{X}^T\hat{X} + \lambda \hat{I}
%\end{equation}
%
%where $I$ is the identity matrix. (cite?).
%This gives
%
%\begin{equation}
%	RSS(\lambda) = (\hat{y}-\hat{X}\hat{\beta)}^T(\hat{y}
%	- \hat{X}\hat{\beta})
%	- \lambda \hat{\beta}^T\hat{\beta}
%\end{equation}
%and
%\begin{equation}
%	\hat{\beta}^{ridge} = (\hat{X}^T\hat{X}
%	+ \lambda \hat{I})^{-1}\hat{X}^T\hat{y}
%\end{equation}
%\(\lambda > 0 \) is a complexity parameter that controls the amount
%of shrinkage. The larger the value of \(\lambda\) the greater amount of
%shrinkage.
%\cite{James2000}
%
%This can also be written in a non-vector format which makes
%explicit the size constraint on the parameters.
%It also makes it easier to see the difference from Lasso regression:
%
%\begin{equation}
%	\hat{\beta}^{ridge} = \operatorname{argmin}_{beta}
%							\sum\limits_{i=1}^n \left(y_i - \beta_0
%							-\sum\limits_{j=1}^p x_{ij}\beta_j\right)^2
%\end{equation}
%subject to
%\begin{equation}
%	\sum\limits_{j=1}^p \beta_j^2 \leq t
%\end{equation}
%There is a one to one correspondence between \(\lambda\) and \(t\).
%\cite{James2000}
\section{Logistic Regression}\label{seq:logistic}
Differently to linear regression, classification problems
are concerned with outcomes taking the form of discrete variables.
For a specific physical problem, we'd like to identify its state, say whether
it is an ordered of disordered system. (cite?) One of the most basic examples
of a classifier algorithm is logistic regression.

For a logistic regressor to improve it needs a way to
track its performance. This is the purpose of a cost function. Essentially,
the cost function says something about how wrong the model is in classifying the
input. The objective in machine learning, and logistic regression, is then to minimize
this error.

The cost function used in this project is called the \textbf{cross-entropy}, or the
'negative log likelihood', and takes the form
\begin{equation}\label{eq:cross-entropy}
	\mathcal{C}(\boldsymbol{w})=-\sum_{i=1}^n  \left(y_i(w_0+w_1x_i) -\log{(1+\exp{(w_0+w_1x_i)})}\right)
\end{equation}

\section{Gradient Descent}\label{seq:gradient}
Minimizing the cost function is done using Gradient Descent.
The gist of it is that to optimize the weights or coefficients,
and biases to minimize the cost function, one can change their values to

\begin{equation}\label{eq:delta-c}
	\frac{\partial \mathcal{C}(\boldsymbol{w})}{\partial \boldsymbol{w}} = -\boldsymbol{X}^T\left(\hat{y}-\hat{p}\right)
\end{equation}

\subsection{Stochastic Gradient Descent}
The stochastic gradient descent method address some of the shortcomings
of the normal gradient descent method. The gradient descent method is
for instance sensitive to the choice of learning rate (cite?).

The underlying idea of stochastic gradient descent comes from observing
that the cost function we want to minimize, almost always can be written as
a sum over \(n\) data points. (cite?). Which gives

\begin{equation}
	C(\beta) = \sum\limits_{i=1}^n c_i (\mathbf{x}_i\beta)
\end{equation} (cite?)

This means that we also can find the gradient as a sum over i gradients
as follows:

\begin{equation}
	\Delta_{\beta} C(\beta) = \sum\limits_{i}^n \Delta_{\beta}c_i (\mathbf{x}_i\beta)
\end{equation} (cite?)

Randomness is included by only taking the gradient on a subset of data.

\section{Neural Networks}
\subsection{Perceptron}

\subsubsection{Multilayer Perceptron}

\subsection{Backpropagation}

\section{Assessing the Performance of Models}
If classification accuracy is not enough to gauge whether a model is
performing well, or well in the desired way, alternative way to measure
performance must be explored. For cases of imbalanced data there are a few
widely used methods that reveal information about the model that the simple
accuracy metric can't.

\subsection{Imbalanced Data in Classification}
The physical world is rarely balanced.

A common challenge in classification is imbalanced data, in which a large
amount of the labeled data belongs to just one or a few of the classes.
For binary classification, if 90\% of the data belongs to one of the classes,
then the classifier is likely to end up placing every single
input in that class, as it will bring its accuracy to 90\%. Technically, this
accuracy is correct, but it's not very useful since the decision isn't at all
affected by the features of the input. Accuracy alone isn't a good enough
measure of performance to reveal this.

\subsection{Confusion Matrix}
A confusion matrix is an n by n matrix containing correct classifications
on the diagonal, and false positives and negatives in the off-diagonal elements.
An example of such a matrix could be the following table:
\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c}
     & True Cat & True Dog & True Rabbit \\
    \hline
    Predicted Cat & \textbf{5} & 2 & 0 \\
    \hline
    Predicted Dog & 3 & \textbf{3} & 2 \\
    \hline
    Predicted Rabbit & 0 & 1 & \textbf{11} \\
\end{tabular}
\caption{Confusion matrix for an example classification where the classes
         are Cat, Dog and Rabbit. Correct classifications in bold.}
\label{tab:confmat-example}
\end{table}
In the table above (\ref{tab:confmat-example}), the diagonal elements
$i = j$ are the correct classifications, while the other elements correspond
to cases where the model predicted class $i$ but should've predicted class $j$.
The confusion matrix thus gives information about false positives and false
negatives, in addition to classification accuracy. This is very useful
in cases where for example false positives can be readily ignored or filtere
later, but false negatives may have severe consequences. An example of this
could be detection of cancer, in which a false positive can be ruled out
from further testing, while a false negative may lead to a patient being sent
home when actually needing help. For a more in-depth look at confusion matrices
see \cite{Fawcett2006}.

\subsection{Receiver Operating Characteristic}
The Receiver Operating Characteristic (ROC) is a widely used measure of a
classifiers performance . The performance is measured as the effect
of the true positive rate (TPR) and the false positive rate (FPR) as a function
of thresholding the positive class. To evaluate the ROC curve for a model,
traditionally the Area Under the Curve (AUC) is used, which ranges from 0
(an ideal "opposite" classifier) to 1.0 (an ideal classifier) with 0.5
indicating a random choice classifier,
For a thorough explanation of ROC curves and the underlying concepts, see \cite{Fawcett2006}.

\section{Nuclear Science}
\subsection{Shell Structure}
\begin{itemize}
	\item Comprehensive and predictive model of atomic nuclei
	\begin{itemize}
		\item Evolving structure of atomic nuclei as a function of protons and neutrons from first principles
	\end{itemize}
	\item Understanding the origin of the elements
	\begin{itemize}
		\item Explosive nucleosynthesis
	\end{itemize}
	\item Use of atomic nuclei to test fundamental symmetries
	\item Search for new applications of isotopes and solutions to societal problems
\end{itemize}

\subsubsection{Nomenclature}
A nucleus $Y$ has $Z$ protons and $N$ neutrons with a mass of $A = Z + N$.
This is written as \ce{^{A}_{Z}Y_{N}}. For a given nucleus there may be several
\begin{itemize}
	\item Isotopes - nuclei with the same number of protons, but varying number of neutrons
	\begin{itemize}
		\item \ce{^{66}Ni}, \ce{^{67}Ni}, \ce{^{68}Ni}, \ce{^{69}Ni}, \ce{^{70}Ni}
	\end{itemize}
	\item Isotones - nuclei with the same number of neutrons, but varying number of protons
	\begin{itemize}
		\item \ce{^{70}Zn}, \ce{^{69}Cu}, \ce{^{68}Ni}, \ce{^{67}Co}, \ce{^{66}Fe}
	\end{itemize}
	\item Isobars - nuclei with the same number of nucleons $A$
	\begin{itemize}
		\item \ce{^{68}Fe}, \ce{^{68}Co}, \ce{^{68}Ni}, \ce{^{68}Cu}, \ce{^{68}Zn}
	\end{itemize}
\end{itemize}