Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychological Review},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATION, STORAGE, MODEL FOR, IN BRAIN BRAIN, INFORMATION STORAGE IN, MODEL FOR LEARNING {\&} MEMORY},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@article{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
file = {:home/ulvik/Downloads/1608.06993.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Koyuncu2019,
abstract = {This paper presents a new deep regression model, which we call DeepDistance, for cell detection in images acquired with inverted microscopy. This model considers cell detection as a task of finding most probable locations that suggest cell centers in an image. It represents this main task with a regression task of learning an inner distance metric. However, different than the previously reported regression based methods, the DeepDistance model proposes to approach its learning as a multi-task regression problem where multiple tasks are learned by using shared feature representations. To this end, it defines a secondary metric, normalized outer distance, to represent a different aspect of the problem and proposes to define its learning as complementary to the main cell detection task. In order to learn these two complementary tasks more effectively, the DeepDistance model designs a fully convolutional network (FCN) with a shared encoder path and end-to-end trains this FCN to concurrently learn the tasks in parallel. DeepDistance uses the inner distances estimated by this FCN in a detection algorithm to locate individual cells in a given image. For further performance improvement on the main task, this paper also presents an extended version of the DeepDistance model. This extended model includes an auxiliary classification task and learns it in parallel to the two regression tasks by sharing feature representations with them. Our experiments on three different human cell lines reveal that the proposed multi-task learning models, the DeepDistance model and its extended version, successfully identify cell locations, even for the cell line that was not used in training, and improve the results of the previous deep learning methods.},
archivePrefix = {arXiv},
arxivId = {1908.11211},
author = {Koyuncu, Can Fahrettin and Gunesli, Gozde Nur and Cetin-Atalay, Rengul and Gunduz-Demir, Cigdem},
eprint = {1908.11211},
file = {:home/ulvik/Downloads/1908.11211.pdf:pdf},
keywords = {cell detection,feature learning,fully convolutional network,inverted microscopy,multi-task learning},
title = {{DeepDistance: A Multi-task Deep Regression Model for Cell Detection in Inverted Microscopy Images}},
url = {http://arxiv.org/abs/1908.11211},
year = {2019}
}
@article{Szegedy2017,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
eprint = {1602.07261},
file = {:home/ulvik/Downloads/1602.07261.pdf:pdf},
journal = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
pages = {4278--4284},
title = {{Inception-v4, inception-ResNet and the impact of residual connections on learning}},
year = {2017}
}
@article{Mehta2019,
abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias–variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton–proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
archivePrefix = {arXiv},
arxivId = {1803.08823},
author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching Hao and Day, Alexandre G.R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
doi = {10.1016/j.physrep.2019.03.001},
eprint = {1803.08823},
file = {:home/ulvik/Downloads/1803.08823.pdf:pdf},
issn = {03701573},
journal = {Physics Reports},
pages = {1--124},
title = {{A high-bias, low-variance introduction to Machine Learning for physicists}},
volume = {810},
year = {2019}
}
@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/ulvik/Downloads/1512.03385.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
volume = {2016-Decem},
year = {2016}
}
@article{Kuchera2019,
abstract = {We evaluate machine learning methods for event classification in the Active-Target Time Projection Chamber detector at the National Superconducting Cyclotron Laboratory (NSCL) at Michigan State University. Currently, events of interest are selected via cuts in the track fitting stage of the analysis workflow. An explicit classification step to single out the desired reaction product would result in more accurate physics results as well as a faster analysis process. We tested binary and multi-class classification methods on data produced by the 46Ar(p,p) experiment run at the NSCL in September 2015. We found that fine-tuning a pre-trained convolutional neural network produced the most successful classifier of proton scattering events in the experimental data, when trained on both experimental and simulated data. We present results from this investigation and conclude with recommendations for event classification in future experiments.},
archivePrefix = {arXiv},
arxivId = {1810.10350},
author = {Kuchera, M. P. and Ramanujan, R. and Taylor, J. Z. and Strauss, R. R. and Bazin, D. and Bradt, J. and Chen, Ruiming},
doi = {10.1016/j.nima.2019.05.097},
eprint = {1810.10350},
file = {:home/ulvik/Downloads/1810.10350.pdf:pdf},
issn = {01689002},
journal = {Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
keywords = {Active targets,Classification,Machine learning,Neural networks,Time projection chamber},
pages = {156--167},
title = {{Machine learning methods for track classification in the AT-TPC}},
volume = {940},
year = {2019}
}
@book{James2000,
abstract = {3'-Azido-2',3'-dideoxythymidine (AZT, 1, zidovudine, RetrovirTM) is used to treat patients with human immunodeficiency virus (HIV) infection. AZT, after conversion to AZT-5'-triphosphate (AZT-TP) by cellular enzymes, inhibits HIV-reverse transcriptase (HIV-RT). The major clinical limitations of AZT are due to clinical toxicities that include bone marrow suppression, hepatic abnormalities and myopathy, absolute dependence on host cell kinase-mediated activation which leads to low activity, limited brain uptake, a short half-life of about one hour in plasma that dictates frequent administration to maintain therapeutic drug levels, low potential for metabolic activation and/or high susceptibility to catabolism, and the rapid development of resistance by HIV-1. These limitations have prompted the development of strategies for designing prodrugs of AZT. A variety of 5'-O-substituted prodrugs of AZT constitute the subject of this review. The drug-design rationale on which these approaches are based is that the ester conjugate will be converted by hydrolysis and/or enzymatic cleavage to AZT or its 5{\&}prime;-monophosphate (AZT-MP). Most prodrug derivatives of AZT have been prepared by derivatization of AZT at its 5'-O position to provide two prominent classes of compounds that encompass: A) 5'-O-carboxylic esters derived from 1) cyclic 5'-O-carboxylic acids such as steroidal 17b-carboxylic acids, 1-adamantanecarboxylic acid, bicyclam carboxylic acid derivatives, O-acetylsalicylic acid, and carbohydrate derivatives, 2) amino acids, 3) 1, 4-dihydro-1-methyl-3-pyridinylcarboxylic acid, 4) aliphatic fatty acid analogs such as myristic acid containing a heteroatom, or without a heteroatom such as stearic acid, and 5) long chain polyunsaturated fatty acid analogs such as retinoic acid, and B) masked phosphates such as 1) phosphodiesters that include monoalkyl or monoaryl phosphate, carbohydrate, ether lipid, ester lipid, and foscarnet derivatives, 2) a variety of phosphotriesters that include dialkylphosphotriesters, diarylphosphotriesters, glycolate and lactate phosphotriesters, phosphotriester approaches using simultaneous enzymatic and chemical hydrolysis of bis(4-acyloxybenzyl) esters, bis(S-acyl-2-thioethyl) (SATE) esters, cyclosaligenyl prodrugs, glycosyl phosphotriesters, and steroidal phosphotriesters, 3) phosphoramidate derivatives, 4) dinucleoside phosphate derivatives that possess a second anti-HIV moiety such as AZT-P-ddA, AZT-P-ddI, AZTP2AZT, AZTP2ACV), and 5) 5'-hydrogen phosphonate and 5'-methylene phosphonate derivatives of AZT. In these prodrugs, the conjugating moiety is linked to AZT via a 5'-O-ester or 5'-O-phosphate group. 5'-O-Substituted AZT prodrugs have been designed with the objectives of improving anti-HIV activity, enhancing blood-brain barrier penetration, modifying pharmacokinetic properties to increase plasma half-life and improving drug delivery with respect to site-specific targeting or drug localization. Bypassing the first phosphorylation step, regulating transport and conferring sustained release of AZT prolong its duration of action, decrease toxicity and improve patient acceptability. The properties of these prodrugs and their anti-HIV activities are now reviewed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
booktitle = {Current medicinal chemistry},
doi = {10.1007/978-1-4614-7138-7},
eprint = {arXiv:1011.1669v3},
file = {:home/ulvik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/James et al. - 2000 - An introduction to Statistical Learning.pdf:pdf},
isbn = {978-1-4614-7137-0},
issn = {0929-8673},
number = {10},
pages = {995--1039},
pmid = {10911016},
title = {{An introduction to Statistical Learning}},
volume = {7},
year = {2000}
}
@article{Lathuiliere2018,
abstract = {Deep learning revolutionized data science, and recently its popularity has grown exponentially, as did the amount of papers employing deep networks. Vision tasks, such as human pose estimation, did not escape from this trend. There is a large number of deep models, where small changes in the network architecture, or in the data pre-processing, together with the stochastic nature of the optimization procedures, produce notably different results, making extremely difficult to sift methods that significantly outperform others. This situation motivates the current study, in which we perform a systematic evaluation and statistical analysis of vanilla deep regression, i.e. convolutional neural networks with a linear regression top layer. This is the first comprehensive analysis of deep regression techniques. We perform experiments on four vision problems, and report confidence intervals for the median performance as well as the statistical significance of the results, if any. Surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modifications in the network architecture. Our results reinforce the hypothesis according to which, in general, a general-purpose network (e.g. VGG-16 or ResNet-50) adequately tuned can yield results close to the state-of-the-art without having to resort to more complex and ad-hoc regression models.},
archivePrefix = {arXiv},
arxivId = {1803.08450},
author = {Lathuili{\`{e}}re, St{\'{e}}phane and Mesejo, Pablo and Alameda-Pineda, Xavier and Horaud, Radu},
doi = {10.1109/TPAMI.2019.2910523},
eprint = {1803.08450},
file = {:home/ulvik/Downloads/1803.08450.pdf:pdf},
issn = {1939-3539},
pages = {1--17},
title = {{A Comprehensive Analysis of Deep Regression}},
url = {http://arxiv.org/abs/1803.08450},
year = {2018}
}
@article{Bulat2016,
abstract = {This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from http://www.cs.nott.ac.uk/ ∼psxab5/.},
archivePrefix = {arXiv},
arxivId = {1609.01743},
author = {Bulat, Adrian and Tzimiropoulos, Georgios},
doi = {10.1007/978-3-319-46478-7_44},
eprint = {1609.01743},
file = {:home/ulvik/Downloads/1609.01743.pdf:pdf},
isbn = {9783319464770},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional Neural Networks,Human pose estimation,Part heatmap regression},
pages = {717--732},
title = {{Human pose estimation via convolutional part heatmap regression}},
volume = {9911 LNCS},
year = {2016}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.04938v3},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.1145/2939672.2939778},
eprint = {arXiv:1602.04938v3},
file = {:home/ulvik/Downloads/1602.04938.pdf:pdf},
isbn = {9781450342322},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135--1144},
title = {{"Why should i trust you?" Explaining the predictions of any classifier}},
volume = {13-17-Augu},
year = {2016}
}
@article{Zoph2018,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the 'NASNet search space') which enables transferability. In our experiments, we search for the best convolutional layer (or 'cell') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a 'NASNet architecture'. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4{\%} error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
doi = {10.1109/CVPR.2018.00907},
eprint = {1707.07012},
file = {:home/ulvik/Downloads/1707.07012.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8697--8710},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
year = {2018}
}
@book{Basdevant2005,
abstract = {This course explores nuclear physics and its applications ranging from the structure of nuclei and their reactions, to astrophysics and cosmology. The physics is introduced with arguments based on simple ideas such as the empirical structure of nuclear forces and its interplay with the Pauli principle and Coulomb repulsion. The book then develops elementary nuclear models and illustrates nuclear systematics with experimental data. Reactions and decays are discussed both phenomenologically and from the point of view of fundamental electro-weak interaction theory. The discussions of fission and fusion emphasize nuclear energy production. This leads directly into nuclear astrophysics and nucleosynthesis. The book ends with a presentation of the latest ideas about cosmology. As a primer this course will lay the foundations for more specialized subjects within the vast domain of nuclear physics as a whole. This book emerged from a series of topical courses the authors delivered at the Ecole Polytechnique and will be useful for advanced undergraduates and for scientists in a variety of fields. {\textcopyright} 2005 Springer Science+Business Media, Inc. All rights reserved.},
author = {Basdevant, Jean Louis and Rich, James and Spiro, Michel},
booktitle = {Fundamentals In Nuclear Physics: From Nuclear Structure to Cosmology},
doi = {10.1007/b106774},
file = {:home/ulvik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Basdevant, Rich, Spiro - 2005 - Fundamentals in nuclear physics From nuclear structure to cosmology.pdf:pdf},
isbn = {0387016724},
pages = {1--515},
publisher = {Springer New York},
title = {{Fundamentals in nuclear physics: From nuclear structure to cosmology}},
year = {2005}
}
@article{Fawcett2006,
abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Fawcett, Tom},
doi = {10.1016/j.patrec.2005.10.010},
file = {:home/ulvik/Downloads/Introduction{\_}to{\_}ROC{\_}analysis.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Classifier evaluation,Evaluation metrics,ROC analysis},
number = {8},
pages = {861--874},
title = {{An introduction to ROC analysis}},
volume = {27},
year = {2006}
}
@article{Sandler2018,
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
archivePrefix = {arXiv},
arxivId = {1801.04381},
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang Chieh},
doi = {10.1109/CVPR.2018.00474},
eprint = {1801.04381},
file = {:home/ulvik/Downloads/1801.04381.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4510--4520},
title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
year = {2018}
}
@article{Nibali2018,
abstract = {We study deep learning approaches to inferring numerical coordinates for points of interest in an input image. Existing convolutional neural network-based solutions to this problem either take a heatmap matching approach or regress to coordinates with a fully connected output layer. Neither of these approaches is ideal, since the former is not entirely differentiable, and the latter lacks inherent spatial generalization. We propose our differentiable spatial to numerical transform (DSNT) to fill this gap. The DSNT layer adds no trainable parameters, is fully differentiable, and exhibits good spatial generalization. Unlike heatmap matching, DSNT works well with low heatmap resolutions, so it can be dropped in as an output layer for a wide range of existing fully convolutional architectures. Consequently, DSNT offers a better trade-off between inference speed and prediction accuracy compared to existing techniques. When used to replace the popular heatmap matching approach used in almost all state-of-the-art methods for pose estimation, DSNT gives better prediction accuracy for all model architectures tested.},
archivePrefix = {arXiv},
arxivId = {1801.07372},
author = {Nibali, Aiden and He, Zhen and Morgan, Stuart and Prendergast, Luke},
eprint = {1801.07372},
file = {:home/ulvik/Downloads/1801.07372.pdf:pdf},
title = {{Numerical Coordinate Regression with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1801.07372},
year = {2018}
}
@article{Szegedy2016,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/ulvik/Downloads/1512.00567.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
volume = {2016-Decem},
year = {2016}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:home/ulvik/Downloads/1409.1556.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--14},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Levi2019,
abstract = {Predicting not only the target but also an accurate measure of uncertainty is important for many machine learning applications and in particular safety-critical ones. In this work we study the calibration of uncertainty prediction for regression tasks which often arise in real-world systems. We show that the existing definition for calibration of a regression uncertainty [Kuleshov et al. 2018] has severe limitations in distinguishing informative from non-informative uncertainty predictions. We propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach. Our method clusters examples with similar uncertainty prediction and compares the prediction with the empirical uncertainty on these examples. We also propose a simple, scaling-based calibration method that preforms as well as much more complex ones. We show results on both a synthetic, controlled problem and on the object detection bounding-box regression task using the COCO and KITTI datasets.},
archivePrefix = {arXiv},
arxivId = {1905.11659},
author = {Levi, Dan and Gispan, Liran and Giladi, Niv and Fetaya, Ethan},
eprint = {1905.11659},
file = {:home/ulvik/Downloads/1905.11659.pdf:pdf},
title = {{Evaluating and Calibrating Uncertainty Prediction in Regression Tasks}},
url = {http://arxiv.org/abs/1905.11659},
year = {2019}
}
@article{Howard2017,
abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
archivePrefix = {arXiv},
arxivId = {1704.04861},
author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
eprint = {1704.04861},
file = {:home/ulvik/Downloads/1704.04861.pdf:pdf},
title = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}},
url = {http://arxiv.org/abs/1704.04861},
year = {2017}
}
@article{Chollet2017,
abstract = {We present an interpretation of Inception modules in con-volutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17, 000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
archivePrefix = {arXiv},
arxivId = {1610.02357},
author = {Chollet, Fran{\c{c}}ois},
doi = {10.1109/CVPR.2017.195},
eprint = {1610.02357},
file = {:home/ulvik/Downloads/1610.02357.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {1800--1807},
title = {{Xception: Deep learning with depthwise separable convolutions}},
volume = {2017-Janua},
year = {2017}
}
