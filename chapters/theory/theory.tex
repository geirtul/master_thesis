\section{Linear Regression}\label{sec:linreg}
Suppose you have a data set $\mathcal{L}$ consisting of the data
$\mathbf{X}_\mathcal{L}=\{(y_i, x_i), i=0\ldots n-1\}$. Each point is associated
with a scalar target $y_i$, and a vector $\bs{x}$ containing values for $p$ input
features. Assuming the target variable $y_i$ is linear in the inputs, it can be
written as a linear function of the features, given by
\begin{equation}\label{eq:ols-target}
y_i = w_0 x_{i,0} + w_1 x_{i,1} + ... + w_{p-1} x_{i, p-1} + \epsilon_i,
\end{equation}
where $\bs{w} = (w_0, w_1, \dots, w_{p-1})^T$ is a vector of length
$p$ containing unknown values, and $\epsilon$ are the errors in our estimate.
This gives us a system of linear equations, which can be written in matrix form as
\begin{equation}\label{eq:ols-target-mat}
  \bs{y} = \bs{Xw} + \bs{}{\epsilon},
\end{equation}
where
\begin{equation}
\vec{X} = \left[
\begin{matrix}
x_{0,0} & x_{0,1} & x_{0,2} & ... & x_{0,p-1}\\
x_{1,0} & x_{1,1} & x_{1,2} & ... & x_{1,p-1}\\
x_{2,0} & x_{2,1} & x_{2,2} & ... & x_{2,p-1}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n-1,0} & x_{n-1,1} & x_{n-1,2} & ... & x_{n-1,p-1}
\end{matrix}
\right]
\end{equation}
The unknown values $\bs{w}$ are commonly referred to as \textit{weights} in machine 
learning literature. To find the best possible weights $\bs{w}$ we want a suitable
quantity to optimize - a \textbf{cost function}, $\mathcal{C}$ (also referred to as an
\textbf{objective function}). An example of such a function is the squared error - or the Euclidian vector norm, defined as
\begin{equation}
	L_2(\bs{x}) =||\bs{x}||_2 = \left(\sum x_i^2\right)^{\frac{1}{2}}.
\end{equation}
From this we define the cost function
\begin{equation}
	\mathcal{C} = || \bs{\hat{y}} - \bs{y} ||_2 ^2.
\end{equation}
In machine learning, it is most common to cast the optimization as a minimization problem
("minimize the cost"). Our task is then to find an approximation
\begin{equation}\label{eq:ols-approximation}
	\bs{\hat{y}} = \bs{Xw}
\end{equation}
which minimizes this cost function.
To find the minimum we need a differentiation. To simplify that process, we rewrite the cost function on matrix form
\begin{align*}
\mathcal{C} &= || \bs{\hat{y}} - \bs{y} ||_2 ^2, \\
\mathcal{C} &= ( \bs{\hat{y}} - \bs{Xw})^T( \bs{\hat{y}} - \bs{Xw}).
\end{align*}
To minimize we take the derivative with respect to the weights $\bs{w}$,
and find the minima by setting the derivative equal to zero
\begin{align}
\nabla _{\bs{w}}\mathcal{C} &= \nabla _{\bs{w}} ( \bs{\hat{y}} - \bs{Xw})^T( \bs{\hat{y}} - \bs{Xw}), \\
&= -2\bs{X}^T\bs{\hat{y}} + 2\bs{X}^T\bs{X}\bs{w}, \\
\bs{0} &= -2\bs{X}^T\bs{\hat{y}} + 2\bs{X}^T\bs{X}\bs{w}, \\
\bs{X}^T\bs{\hat{y}} &= \bs{X}^T\bs{X}\bs{w}, \\
\bs{w} &=(\bs{X}^T\bs{X})^{-1} \bs{X}^T\bs{\hat{y}}. \label{eq:ols}
\end{align}
This requires matrix \(\vec{X}^T\vec{X}\) to be invertible to get the solution 
\cite{James2000}.

The residuals $\bs{\epsilon}$ are given by
$$\bs{\epsilon} = \bs{y}-\bs{\hat{y}} = \bs{y}-\bs{X}\bs{w},$$
and with 
$$\bs{X}^T\left( \bs{y}-\bs{X}\bs{w}\right)= 0,$$
we have
$$\bs{X}^T\bs{\epsilon}=\bs{X}^T\left( \bs{y}-\bs{X}\bs{w}\right)= 0,$$
meaning that the solution for $\bs{w}$ is the one which minimizes the residuals.
This method of regression is known as Ordinary Least Squares.

\section{Over- and underfitting}\label{sec:overfitting}
In machine learning, when fitting a model to a data set the goal is nearly always to predict 
values or classify samples from regions of data the model has not seen. This is not a simple task,
especially taking into consideration that data is rarely, if ever, noiseless. When extrapolating to 
unseen regions we must take steps to ensure the model complexity is appropriate - we want it to fit 
the signal, not the noise. First off - what do the terms "overfit" and "underfit" mean?
An overfit model will typically perform well during the fitting procedure, but when presented with
data outside the fitted region its performance decreases considerably.
An underfit model lacks the expressive power to capture core signal variations in the data.
Mehta et. al \cite{Mehta2019} demonstrates this concept through polynomial regression.

In machine learning literature and practice, you will encounter the concept of
splitting the available data into two - training data and test data. We fit, or 'train'
the model on the training data, and then assess the performance of the model on the 
test data. This practice lets us evaluate whether the model is overfitting to unseen
data by comparing performance on the training data and test data.

\section{The Bias-Variance Tradeoff}\label{sec:bias-variance}
Considering the same dataset $\mathcal{L}$ consisting of the data,
let us assume that the true data is generated from a noisy model

$$\mathbf{y}=f(\bs{x}) + \bs{\epsilon},$$
where $\epsilon$ is normally distributed with mean zero and standard deviation $\sigma^2$.

In our derivation of the ordinary least squares method we defined
an approximation (\ref{eq:ols-approximation}) to the function $f$ in terms of the 
weights $\bs{w}$ and the input matrix $\bs{X}$ which together define our model,
that is $\bs{\hat{y}}=\bs{X}\bs{w}$. 

Thereafter we found the parameters $\bs{w}$ by optimizing the means squared error via the so-called cost function

$$\mathcal{C}(\bs{X},\bs{w}) =\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\hat{y}_i)^2=\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right].$$

We can rewrite this as 
$$\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right]=\frac{1}{n}\sum_i(f_i-\mathbb{E}\left[\bs{\hat{y}}\right])^2+\frac{1}{n}\sum_i(\hat{y}_i-\mathbb{E}\left[\bs{\hat{y}}\right])^2+\sigma^2.$$

The three terms represent the square of the bias of the learning
method, which can be thought of as the error caused by the simplifying
assumptions built into the method. The second term represents the
variance of the chosen model and finally the last terms is variance of
the error $\bs{\epsilon}$.

For a derivation of this equation, we refer to the work of Mehta et. al \cite{Mehta2019}.
The authors also provide a list of universal lessons to keep in mind, which we quote here:

\begin{itemize}
	\item Fitting is not predicting. Fitting existing data well is fundamentally
	different from making predictions about new data.
	\item Using a complex model can result in overfitting. Increasing a model's
	complexity  (i.e number of fitting parameters) will usually yield better results
	on the training data. However when the training data size is small and the data
	are noisy, this results in \textit{overfitting} and can substantially degrade the
	predictive performance of the model.
	\item For complex datasets and small training sets, simple models can be better
	at prediction complex models due to the bias-variance tradeoff. It takes less
	data to train a simple model than a complex one. Therefore, even though the 
	correct model is guaranteed to have better predictive performance for an infinite
	amount of training data (less bias), the training errors stemming from finite-size
	sampling (variance) can cause simpler models to outperform the more complex model
	when sampling is limited.

\end{itemize}

% Derivation from lecture notes here.
%to recall that the variance of $\bs{y}$ and $\bs{\epsilon}$ are both equal to $\sigma^2$. 
%The mean value of $\bs{\epsilon}$ is by definition equal to zero. Furthermore, the function $f$ is not a stochastics variable, idem for $\bs{\hat{y}}$.
%We use a more compact notation in terms of the expectation value
%$$\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right]=\mathbb{E}\left[(\bs{f}+\bs{\epsilon}-\bs{\hat{y}})^2\right],$$
%and adding and subtracting $\mathbb{E}\left[\bs{\hat{y}}\right]$ we get
%$$\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right]=\mathbb{E}\left[(\bs{f}+\bs{\epsilon}-\bs{\hat{y}}+\mathbb{E}\left[\bs{\hat{y}}\right]-\mathbb{E}\left[\bs{\hat{y}}\right])^2\right],$$
%which, using the abovementioned expectation values can be rewritten as
%$$\mathbb{E}\left[(\bs{y}-\bs{\hat{y}})^2\right]=\mathbb{E}\left[(\bs{y}-\mathbb{E}\left[\bs{\hat{y}}\right])^2\right]+\mathrm{Var}\left[\bs{\hat{y}}\right]+\sigma^2,$$
%that is the rewriting in terms of the so-called bias, 
%the variance of the model $\bs{\hat{y}}$ and the variance of $\bs{\epsilon}$.

\section{Regularization}\label{section:regularization}
With the computing resources available today, increasing model complexity to deal 
with underfitting is usually a simple task. However, this computational freedom 
has led to overfitting being the common challenge to overcome.

\section{Logistic Regression}\label{seq:logistic}
Differently to linear regression, classification problems
are concerned with outcomes taking the form of discrete variables.
For a specific physical problem, we'd like to identify its state, say whether
it is an ordered of disordered system. (cite?) One of the most basic examples
of a classifier algorithm is logistic regression.

For a logistic regressor to improve it needs a way to
track its performance. This is the purpose of a cost function. Essentially,
the cost function says something about how wrong the model is in classifying the
input. The objective in machine learning, and logistic regression, is then to minimize
this error.

The cost function used in this project is called the \textbf{cross-entropy}, or the
'negative log likelihood', and takes the form
\begin{equation}\label{eq:cross-entropy}
	\mathcal{C}(\bs{w})=-\sum_{i=1}^n  \left(y_i(w_0+w_1x_i) -\log{(1+\exp{(w_0+w_1x_i)})}\right)
\end{equation}

\section{Gradient Descent}\label{seq:gradient}
Minimizing the cost function is done using Gradient Descent.
The gist of it is that to optimize the weights or coefficients,
and biases to minimize the cost function, we can change their values to

\begin{equation}\label{eq:delta-c}
	\frac{\partial \mathcal{C}(\bs{w})}{\partial \bs{w}} = -\bs{X}^T\left(\hat{y}-\hat{p}\right)
\end{equation}

\subsection{Stochastic Gradient Descent}
The stochastic gradient descent method address some of the shortcomings
of the normal gradient descent method. The gradient descent method is
for instance sensitive to the choice of learning rate (cite?).

The underlying idea of stochastic gradient descent comes from observing
that the cost function we want to minimize, almost always can be written as
a sum over \(n\) data points. (cite?). Which gives

\begin{equation}
	C(w) = \sum\limits_{i=1}^n c_i (\mathbf{x}_iw)
\end{equation} (cite?)

This means that we also can find the gradient as a sum over i gradients
as follows:

\begin{equation}
	\Delta_{w} C(w) = \sum\limits_{i}^n \Delta_{w}c_i (\mathbf{x}_iw)
\end{equation} (cite?)

Randomness is included by only taking the gradient on a subset of data.

\section{Neural Networks}
Artificial neural networks (ANN) are computational systems that can learn to
perform tasks by considering examples, generally without being
programmed with any task-specific rules. It is supposed to mimic a
biological system, wherein neurons interact by sending signals in the
form of mathematical functions between layers. All layers can contain
an arbitrary number of neurons, and each connection is represented by
a weight variable.
The field of artificial neural networks has a long history of
development, and is closely connected with the advancement of computer
science and computers in general.

In natural sciences, ANNs have already found numerous applications.
In statistical physics, they have been applied to detect
phase transitions in 2D Ising and Potts models, lattice gauge
theories, and different phases of polymers, or solving the
Navier-Stokes equation in weather forecasting.  Deep learning has also
found interesting applications in quantum and nuclear physics.

The applications are not limited to the natural sciences. There is a
plethora of applications in essentially all disciplines, from the
humanities to life science and medicine.

\subsection{Artificial Neurons}
A model of artificial neurons was first developed by McCulloch and Pitts in 1943 
\cite{McCulloch1943} to study signal
processing in the brain and has later been refined by others. The
general idea is to mimic neural networks in the human brain, which is
composed of billions of neurons that communicate with each other by
sending electrical signals.  Each neuron accumulates its incoming
signals, which must exceed an activation threshold to yield an
output. If the threshold is not overcome, the neuron remains inactive,
i.e. has zero output. This behaviour inspired a simple mathematical model for an artificial neuron.

\begin{equation}\label{eq:artificial-neuron}
 y = f\left(\sum_{i=1}^n w_ix_i\right) = f(z)
\end{equation}

Here, the output $y$ of the neuron is the value of its activation function,
which receives as input a weighted sum of signals $x_i, \dots ,x_n$ received by $n$ other neurons.
Neurons are often referred to as "nodes" or "units" in machine learning literature,
and we will use these interchangeably in the following sections.

\subsection{The Feed Forward Neural Network}
A network of only one neuron such as the one described above is typically referred
to as a \textit{perceptron}. The simplest network structure contains a
single layer of N such nodes, and is most often called a \textit{single-layer perceptron}.
Adding additional layers of nodes, so-called \textit{hidden layers}, results in a
type of feed-forward neural network (FFNN), typically referred to as a Multilayer Perceptron (MLP)
(see figure \ref{fig:theory-mlp}). The example is also a fully connected network, as every
node in a layer is connected to every node in the next. The name "feed-forward" stems from
the fact that information flows in only one direction: forward through the layers.
\input{chapters/theory/figures/neural_network_example.tex}
First, for each node $i$ in the first hidden layer, we calculate a weighted sum $z_i^1$ of the input coordinates $x_j$,

\begin{equation} 
	z_i^1 = \sum_{j=1}^{M} w_{ij}^1 x_j + b_i^1
\end{equation}
Here $b_i$ is the \textit{bias} which is needed in
case of zero activation weights or inputs. How to fix the biases and
the weights will be discussed below.  The value of $z_i^1$ is the
argument to the activation function $f_i$ of each node $i$, The
variable $M$ stands for all possible inputs to a given node $i$ in the
first layer.  We define  the output $y_i^1$ of all neurons in layer 1 as

\begin{equation}\label{eq:output-layer}
	y_i^1 = f(z_i^1) = f\left(\sum_{j=1}^M w_{ij}^1 x_j  + b_i^1\right)
\end{equation}
where we assume that all nodes in the same layer have identical
activation functions, hence the notation $f$. In general, we could assume in the more general case that different layers have different activation functions.
In this case we would identify these functions with a superscript $l$ for the $l$-th layer,

\begin{equation}\label{eq:general-layer}
	y_i^l = f^l(u_i^l) = f^l\left(\sum_{j=1}^{N_{l-1}} w_{ij}^l y_j^{l-1} + b_i^l\right)
\end{equation}
where $N_l$ is the number of nodes in layer $l$. When the output of
all the nodes in the first hidden layer are computed, the values of
the subsequent layer can be calculated and so forth until the output
is obtained.

The output of neuron $i$ in layer 2 is thus,

\begin{align}\label{eq:output-layer-2}
	y_i^2 &= f^2\left(\sum_{j=1}^N w_{ij}^2 y_j^1 + b_i^2\right) \\
	&= f^2\left[\sum_{j=1}^N w_{ij}^2f^1\left(\sum_{k=1}^M w_{jk}^1 x_k + b_j^1\right) + b_i^2\right]
\end{align}
where we have substituted $y_k^1$ with the inputs $x_k$. Finally, the ANN output reads

\begin{align}
	y_i^3 &= f^3\left(\sum_{j=1}^N w_{ij}^3 y_j^2 + b_i^3\right) \\
	&= f_3\left[\sum_{j} w_{ij}^3 f^2\left(\sum_{k} w_{jk}^2 f^1\left(\sum_{m} w_{km}^1 x_m + b_k^1\right) + b_j^2\right)
	+ b_1^3\right]
\end{align}

We can generalize this expression to an MLP with $l$ hidden
layers. The complete functional form is,

\begin{align}\label{eq:complete-nn}
&y^{l+1}_i = f^{l+1}\left[\!\sum_{j=1}^{N_l} w_{ij}^3 f^l\left(\sum_{k=1}^{N_{l-1}}w_{jk}^{l-1}\left(\dots f^1\left(\sum_{n=1}^{N_0} w_{mn}^1 x_n+ b_m^1\right)\dots\right)+b_k^2\right)+b_1^3\right] &&
\end{align}

which illustrates a basic property of MLPs: The only independent
variables are the input values $x_n$.
This confirms that an MLP, despite its quite convoluted
mathematical form, is nothing more than an analytic function, specifically a
mapping of real-valued vectors $\bs{x} \in \mathbb{R}^n \rightarrow
\bs{y} \in \mathbb{R}^m$.

Furthermore, the flexibility and universality of an MLP can be
illustrated by realizing that the expression is essentially a nested
sum of scaled activation functions of the form
\begin{equation}
 f(x) = c_1 f(c_2 x + c_3) + c_4
\end{equation}

where the parameters $c_i$ are weights and biases. By adjusting these
parameters, the activation functions can be shifted up and down or
left and right, change slope or be rescaled which is the key to the
flexibility of a neural network.

We will now introduce a more convenient notation for the activations in an ANN. 
We can represent the biases and activations
as layer-wise column vectors $\bs{b}_l$ and $\bs{y}_l$, so that the $i$-th element of each vector 
is the bias $b_i^l$ and activation $y_i^l$ of node $i$ in layer $l$ respectively. 

We have that $\bs{W}_l$ is an $N_{l-1} \times N_l$ matrix, 
while $\bs{b}_l$ and $\bs{y}_l$ are $N_l \times 1$ column vectors. 
With this notation, the sum becomes a matrix-vector multiplication, and we can write
the equation for the activations of hidden layer 2 (assuming three nodes for simplicity) as
\begin{equation}
 \bs{y}_2 = f_2(\bs{W}_2 \bs{y}_{1} + \bs{b}_{2}) = 
 f_2\left(\left[\begin{array}{ccc}
    w^2_{11} &w^2_{12} &w^2_{13} \\
    w^2_{21} &w^2_{22} &w^2_{23} \\
    w^2_{31} &w^2_{32} &w^2_{33} \\
    \end{array} \right] \cdot
    \left[\begin{array}{c}
           y^1_1 \\
           y^1_2 \\
           y^1_3 \\
          \end{array}\right] + 
    \left[\begin{array}{c}
           b^2_1 \\
           b^2_2 \\
           b^2_3 \\
          \end{array}\right]\right).
\end{equation}

The activation of node $i$ in layer 2 is
\begin{equation}
 y^2_i = f_2\Bigr(w^2_{i1}y^1_1 + w^2_{i2}y^1_2 + w^2_{i3}y^1_3 + b^2_i\Bigr) = 
 f_2\left(\sum_{j=1}^3 w^2_{ij} y_j^1 + b^2_i\right).
\end{equation}

This is not just a convenient and compact notation, but also a useful
and intuitive way to think about MLPs: The output is calculated by a
series of matrix-vector multiplications and vector additions that are
used as input to the activation functions. For each operation
$\bs{W}_l \bs{y}_{l-1}$ we move forward one layer.

\subsection{Activation Functions}
Other than its connectivity, the choice of which activation function(s) to employ 
is one of the defining properties of a neural network. Not just any function will do,
however, and there are several restrictions imposed on any applicable function.
An activation function for an FFNN must be
\begin{itemize}
	\item Non-constant
	\item Bounded
	\item Monotonically-increasing
	\item Continuous
\end{itemize}

\noindent As linear functions are not bounded, the second requirement exludes this entire family of functions.
The output of a neural network with linear activation functions would be nothing more than a linear function
of the inputs. We need to introduce some form of non-linearity to be able to fit non-linear functions.
The most common examples of such functions are the logistic \textit{sigmoid}
\begin{equation}\label{eq:sigmoid}
	\sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}
and the \textit{hyperbolic tangent}
\begin{equation}\label{eq:hyp-tangent}
	f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}.
\end{equation}

\noindent In addition to meeting the requirements, these functions also have derivatives that are relatively cheap
to compute. The sigmoid's derivative is
\begin{equation}
	\ddx{\sigma(x)} = \sigma(x)(1-\sigma(x)),
\end{equation}
and the hyperbolic tangents is
\begin{equation}
	\ddx{tanh(x)} = 1 - tanh^2(x)
\end{equation}
However, the sigmoid functions suffer from saturating gradients. This occurs when the functions value
changes little to nothing with changes in the value of x. This has lead to a continuous search for
alternatives, and one of the most popular activation functions to day is the \textit{Rectified Linear Unit} (ReLU).
The function, made especially popular after the success of Krizhevsky et al. \cite{Krizhevsky2012},
takes the following form
\begin{equation}\label{eq:relu}
	\text{ReLU} (x) = f(x) = \begin{cases}
	x, & \text{if } x > 0 \\
	0,  & \text{otherwise} .
	\end{cases}
\end{equation}

\noindent This function is certainly monotonic, and we can approximate its derivative with the Heaviside
step-function, denoted $H(x)$ on the following form
\begin{equation}\label{eq:heaviside}
	H(x) = f'(x) = 	\begin{cases}1, & \text{if } x > 0 \\
		0,  & \text{otherwise}.
	\end{cases}
\end{equation}

\subsection{Backpropagation}

As we have seen now in a feed-forward network, we can express the final output of our 
network in terms of basic matrix-vector multiplications. The unknown quantities are 
our weights $w_{ij}$ and we need to find an algorithm for changing them so that our 
errors are as small as possible. This leads us to the famous backpropagation algorithm
\cite{Rumelhart1986}.

The questions we want to ask are how do changes in the biases and the
weights in our network change the cost function and how can we use the
final output to modify the weights?
To derive these equations let us start with a plain regression problem
and define our cost function 

$$\mathcal{C}(\bs{W})  =  \frac{1}{2}\sum_{i=1}^n\left(\hat{y_i} - y_i\right)^2,$$

where the $y_i$'s are our $n$ targets (the values we want to
reproduce), while the outputs of the network after having propagated
all inputs $\hat{x}$ are given by $y_i$.  Below we will demonstrate
how the basic equations arising from the backpropagation algorithm
can be modified to study classification problems with $K$ classes.

With our definition of the targets $\bs{y}$, the outputs of the
network $\hat{\bs{y}}$ and the inputs $\bs{x}$ we
define now the activation $z_j^l$ of node $j$ of the
$l$-th layer as a function of the bias, the weights which add up from
the previous layer $l-1$ and the outputs
$\bs{a}^{l-1}$ from the previous layer as

$$z_j^l = \sum_{i=1}^{M_{l-1}}w_{ij}^la_i^{l-1}+b_j^l,$$

where $b_k^l$ are the biases from layer $l$.  Here $M_{l-1}$
represents the total number of nodes of layer $l-1$. 
We can rewrite this in a more compact form as the matrix-vector products 
we discussed earlier,

$$\bs{z}^l = \left(\bs{W}^l\right)^T\bs{a}^{l-1}+\bs{b}^l.$$

With the activation values $\bs{z}^l$ we can in turn define the
output of layer $l$ as $\bs{a}^l = f(\bs{z}^l)$ where $f$ is our
activation function. In the examples here we will use the sigmoid
function discussed in our logistic regression lectures. We will also use the same activation function $f$ for all layers
and their nodes.  It means we have

$$a_j^l = f(z_j^l) = \frac{1}{1+e^{-(z_j^l)}}.$$

\noindent From the definition of the activation $z_j^l$ we have

$$\frac{\partial z_j^l}{\partial w_{ij}^l} = a_i^{l-1},$$
and
$$\frac{\partial z_j^l}{\partial a_i^{l-1}} = w_{ji}^l.$$

\noindent With our definition of the activation function we have that (note that this function depends only on $z_j^l$)

$$\frac{\partial a_j^l}{\partial z_j^{l}} = a_j^l(1-a_j^l)=f(z_j^l)(1-f(z_j^l)).$$

\noindent With these definitions we can now compute the derivative of the cost function in terms of the weights.
Let us specialize to the output layer $l=L$. Our cost function is
$$\mathcal{C}(\bs{W}^L)  =  \frac{1}{2}\sum_{i=1}^n\left(\hat{y}_i - y_i\right)^2=\frac{1}{2}\sum_{i=1}^n\left(a_i^L - y_i\right)^2,$$
The derivative of this function with respect to the weights is
$$\frac{\partial \mathcal{C}(\bs{W}^L)}{\partial w_{jk}^L}  =  \left(a_j^L - t_j\right)\frac{\partial a_j^L}{\partial w_{jk}^{L}},$$
The last partial derivative can easily be computed and reads (by applying the chain rule)
$$\frac{\partial a_j^L}{\partial w_{jk}^{L}} = \frac{\partial a_j^L}{\partial z_{j}^{L}}\frac{\partial z_j^L}{\partial w_{jk}^{L}}=a_j^L(1-a_j^L)a_k^{L-1},$$
We have thus
$$\frac{\partial \mathcal{C}(\bs{W}^L)}{\partial w_{jk}^L}  =  \left(a_j^L - t_j\right)a_j^L(1-a_j^L)a_k^{L-1},$$

\noindent Defining

$$\delta_j^L = a_j^L(1-a_j^L)\left(a_j^L - t_j\right) = f'(z_j^L)\frac{\partial {\cal C}}{\partial (a_j^L)},$$

and using the Hadamard product of two vectors we can write this as

\begin{equation}\label{eq:bprop-error}
	\bs{\delta}^L = f'(\bs{z}^L)\circ\frac{\partial \mathcal{C}}{\partial (\bs{a}^L)}.
\end{equation}

\noindent This is an important expression. The second term on the right-hand side
measures how fast the cost function is changing as a function of the $j$th
output activation.  If, for example, the cost function doesn't depend
much on a particular output node $j$, then $\delta_j^L$ will be small,
which is what we would expect. The first term on the right measures
how fast the activation function $f$ is changing at a given activation
value $z_j^L$.

\noindent Notice that everything in the above equations is easily computed.  In
particular, we compute $z_j^L$ while computing the behaviour of the
network, and it is only a small additional overhead to compute
$f'(z^L_j)$.  The exact form of the derivative with respect to the
output depends on the form of the cost function.
However, provided the cost function is known there should be little
trouble in calculating

$$\frac{\partial \mathcal{C}}{\partial (a_j^L)}$$

\noindent With the definition of $\delta_j^L$ we have a more compact definition of the derivative of the cost function in terms of the weights, namely
\begin{equation}\label{eq:cost-derivative-compact}
	\frac{\partial{\cal C}(\hat{W^L})}{\partial w_{jk}^L}  =  \delta_j^La_k^{L-1}.
\end{equation}
It is now possible to rewrite our previous equation for $\delta_j^L$ (\ref{eq:bprop-error}) as
\begin{equation}
	\delta_j^L =\frac{\partial \mathcal{C}}{\partial z_j^L}= \frac{\partial \mathcal{C}}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L},
\end{equation}
which can also be interpreted as the partial derivative of the cost function with respect to the biases $b_j^L$, namely

$$\delta_j^L = \frac{\partial\mathcal{C}}{\partial b_j^L}\frac{\partial b_j^L}{\partial z_j^L}=\frac{\partial\mathcal{C}}{\partial b_j^L},$$
That is, the error $\delta_j^L$ is exactly equal to the rate of change of the cost function as a function of the bias.

We now have three equations that are essential for the computations of the derivatives of the cost function at the output layer.
These equations are needed to start the algorithm and they are

\begin{equation}
\frac{\partial\mathcal{C}(\bs{W}^L)}{\partial w_{jk}^L}  =  \delta_j^La_k^{L-1},
\end{equation}
and
\begin{equation}
\delta_j^L = f'(z_j^L)\frac{\partial {\cal C}}{\partial a_j^L},
\end{equation}
and
\begin{equation}
\delta_j^L = \frac{\partial\mathcal{C}}{\partial b_j^L},
\end{equation}

\noindent An interesting consequence of the above equations is that when the
activation $a_k^{L-1}$ is small, the gradient term, that is the
derivative of the cost function with respect to the weights, will also
tend to be small. From this we gather that the weight changes (or "learns") slowly when we
minimize the weights via gradient descent. 

Another feature is that when the activation function (in this case sigmoid),
is rather flat when we move towards its limit values $0$ and $1$. In these
cases, the derivatives of the activation function will also be close
to zero, meaning again that the gradients will be small and the
network learns slowly again.

\noindent We need a fourth equation and we are set. We are going to propagate
backwards to determine the weights and biases. To do so we need to represent
the error in the layer before the final one $L-1$ in terms of the errors in the 
final output layer. Replacing the final layer $L$ with a general layer $l$, we have
$$\delta_j^l =\frac{\partial\mathcal{C}}{\partial z_j^l}.$$
We want to express this in terms of the equations for layer $l+1$.
Using the chain rule and summing over all $k$ entries we have

$$\delta_j^l =\sum_k \frac{\partial\mathcal{C}}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^{l}}=\sum_k \delta_k^{l+1}\frac{\partial z_k^{l+1}}{\partial z_j^{l}},$$
and recalling that
$$z_j^{l+1} = \sum_{i=1}^{M_{l}}w_{ij}^{l+1}a_i^{l}+b_j^{l+1},$$
with $M_l$ being the number of nodes in layer $l$, we arrive at
\begin{equation}\label{eq:backprop-general}
	\delta_j^l =\sum_k \delta_k^{l+1}w_{kj}^{l+1}f'(z_j^l),
\end{equation}
The four equations provide us with a way of computing the gradient of the cost function.

First, we set up the input data $\bs{x}$ and the activations
$\bs{z}_1$ of the input layer and compute the activation function and
the pertinent outputs $\bs{a}^1$.

Secondly, we perform the feed-forward until we reach the output
layer and compute all $\bs{z}_l$ of the input layer and compute the
activation function and the pertinent outputs $\bs{a}^l$ for
$l=2,3,\dots,L$.
Next we compute the ouput error $\bs{\delta}^L$ by computing all
$$\delta_j^L = f'(z_j^L)\frac{\partial\mathcal{C}}{\partial a_j^L}.$$
Then we compute the back propagate error for each $l=L-1,L-2,\dots,2$ as
$$\delta_j^l = \sum_k \delta_k^{l+1}w_{kj}^{l+1}f'(z_j^l).$$
Finally, we update the weights and the biases using gradient descent for 
each $l=L-1,L-2,\dots,2$ and update the weights and biases according to the rules
$$w_{jk}^l\leftarrow  = w_{jk}^l- \eta \delta_j^la_k^{l-1},$$

$$b_j^l \leftarrow b_j^l-\eta \frac{\partial {\cal C}}{\partial b_j^l}=b_j^l-\eta \delta_j^l,$$

\noindent The parameter $\eta$ is the learning parameter discussed in connection with the 
gradient descent methods.

\section{Assessing the Performance of Models}
If classification accuracy is not enough to gauge whether a model is
performing well, or well in the desired way, alternative way to measure
performance must be explored. For cases of imbalanced data there are a few
widely used methods that reveal information about the model that the simple
accuracy metric can't.

\subsection{Imbalanced Data in Classification}
The physical world is rarely balanced.

A common challenge in classification is imbalanced data, in which a large
amount of the labeled data belongs to just one or a few of the classes.
For binary classification, if 90\% of the data belongs to one of the classes,
then the classifier is likely to end up placing every single
input in that class, as it will bring its accuracy to 90\%. Technically, this
accuracy is correct, but it's not very useful since the decision isn't at all
affected by the features of the input. Accuracy alone isn't a good enough
measure of performance to reveal this.

\subsection{Confusion Matrix}
A confusion matrix is an n by n matrix containing correct classifications
on the diagonal, and false positives and negatives in the off-diagonal elements.
An example of such a matrix could be the following table:
\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c}
     & True Cat & True Dog & True Rabbit \\
    \hline
    Predicted Cat & \textbf{5} & 2 & 0 \\
    \hline
    Predicted Dog & 3 & \textbf{3} & 2 \\
    \hline
    Predicted Rabbit & 0 & 1 & \textbf{11} \\
\end{tabular}
\caption{Confusion matrix for an example classification where the classes
         are Cat, Dog and Rabbit. Correct classifications in bold.}
\label{tab:confmat-example}
\end{table}
In the table above (\ref{tab:confmat-example}), the diagonal elements
$i = j$ are the correct classifications, while the other elements correspond
to cases where the model predicted class $i$ but should've predicted class $j$.
The confusion matrix thus gives information about false positives and false
negatives, in addition to classification accuracy. This is very useful
in cases where for example false positives can be readily ignored or filtere
later, but false negatives may have severe consequences. An example of this
could be detection of cancer, in which a false positive can be ruled out
from further testing, while a false negative may lead to a patient being sent
home when actually needing help. For a more in-depth look at confusion matrices
see \cite{Fawcett2006}.

\subsection{Receiver Operating Characteristic}
The Receiver Operating Characteristic (ROC) is a widely used measure of a
classifiers performance . The performance is measured as the effect
of the true positive rate (TPR) and the false positive rate (FPR) as a function
of thresholding the positive class. To evaluate the ROC curve for a model,
traditionally the Area Under the Curve (AUC) is used, which ranges from 0
(an ideal "opposite" classifier) to 1.0 (an ideal classifier) with 0.5
indicating a random choice classifier,
For a thorough explanation of ROC curves and the underlying concepts, see \cite{Fawcett2006}.

\section{Nuclear Science}
\subsection{Shell Structure}
\begin{itemize}
	\item Comprehensive and predictive model of atomic nuclei
	\begin{itemize}
		\item Evolving structure of atomic nuclei as a function of protons and neutrons from first principles
	\end{itemize}
	\item Understanding the origin of the elements
	\begin{itemize}
		\item Explosive nucleosynthesis
	\end{itemize}
	\item Use of atomic nuclei to test fundamental symmetries
	\item Search for new applications of isotopes and solutions to societal problems
\end{itemize}

\subsubsection{Nomenclature}
A nucleus $Y$ has $Z$ protons and $N$ neutrons with a mass of $A = Z + N$.
This is written as \ce{^{A}_{Z}Y_{N}}. For a given nucleus there may be several
\begin{itemize}
	\item Isotopes - nuclei with the same number of protons, but varying number of neutrons
	\begin{itemize}
		\item \ce{^{66}Ni}, \ce{^{67}Ni}, \ce{^{68}Ni}, \ce{^{69}Ni}, \ce{^{70}Ni}
	\end{itemize}
	\item Isotones - nuclei with the same number of neutrons, but varying number of protons
	\begin{itemize}
		\item \ce{^{70}Zn}, \ce{^{69}Cu}, \ce{^{68}Ni}, \ce{^{67}Co}, \ce{^{66}Fe}
	\end{itemize}
	\item Isobars - nuclei with the same number of nucleons $A$
	\begin{itemize}
		\item \ce{^{68}Fe}, \ce{^{68}Co}, \ce{^{68}Ni}, \ce{^{68}Cu}, \ce{^{68}Zn}
	\end{itemize}
\end{itemize}